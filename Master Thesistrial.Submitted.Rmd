---
title: ''
output:
  pdf_document:
    toc: yes
    toc_depth: 2
    number_sections: no
  word_document:
    toc: yes
    toc_depth: '2'
indent: yes
geometry: left = 2.5cm, right = 2.5cm, top = 2.5cm, bottom = 2.5cm
fontsize: 11pt
header-includes:
- \usepackage{float}
- \usepackage{sectsty}
- \usepackage{paralist}
- \usepackage{footnote}
- \usepackage{setspace}\spacing{1.5}
- \usepackage{fancyhdr}
- \usepackage{lastpage}
- \usepackage{dcolumn}
- \usepackage{titlesec}
- \usepackage{caption}
- \usepackage[nottoc]{tocbibind}
- \usepackage[document]{ragged2e}
bibliography: ref.bib
csl: apa.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tinytex)
library(tidyverse)
library(readxl)
library(psych)
library(car)
library(dplyr)
library(broom)
library(ggplot2)
library(showtext)
library(pwr)
library(tidyr)
library(cowplot)
library(stats)
library(ggpubr)
library(visreg)
library(gridExtra)
library(cowplot)
library(rstatix) 
library(coin)
library(survey)
library(effsize)
library(pROC)
library(lme4)
```

\pagenumbering{roman}
\setcounter{page}{5}

\newpage

\thispagestyle{empty}

\begin{centering}

\vspace{1cm}

\Large{KU LEUVEN}

\vspace{1cm}

\Large{FACULTY OF PSYCHOLOGY AND\\
EDUCATIONAL SCIENCES}

\vspace{4cm}

\Large

\doublespacing
{\bf HOW EXPERTISE CHANGES REPRESENTATIONAL GEOMETRY:\\
THE CASE OF CHESS}

\vspace{2 cm}

\normalsize
\singlespacing

\end{centering}

\vspace{1.5 cm}

\begin{flushright}
Master's thesis submitted for the\\
degree of Master of Science in\\
Master of Psychology: Theory and\\
Research by\\
\textbf{Felipe Fontana Vieira.}\\
\vspace{0.5 cm}
Supervisor: Hans Op de Beeck
\end{flushright}

\vspace{2.5 cm}

\begin{centering}

\normalsize
2021-2023

\end{centering}

\newpage

\

\thispagestyle{empty}

\newpage

\thispagestyle{empty}

\begin{centering}

\vspace{1cm}

\Large{KU LEUVEN}

\vspace{1cm}

\Large{FACULTY OF PSYCHOLOGY AND\\
EDUCATIONAL SCIENCES}

\vspace{4cm}

\Large

\doublespacing
{\bf HOW EXPERTISE CHANGES REPRESENTATIONAL GEOMETRY:\\
THE CASE OF CHESS}

\vspace{2 cm}

\normalsize
\singlespacing

\end{centering}

\vspace{1.5 cm}

\begin{flushright}
Master's thesis submitted for the\\
degree of Master of Science in\\
Master of Psychology: Theory and\\
Research by\\
\textbf{Felipe Fontana Vieira.}\\
\vspace{0.5 cm}
Supervisor: Hans Op de Beeck
\end{flushright}

\vspace{2.5 cm}

\begin{centering}

\normalsize
2021-2023

\end{centering}

\newpage

\pagenumbering{roman}

\sectionfont{\centering}
\begingroup
\Large
\bfseries
\section*{Summary}
\addcontentsline{toc}{section}{Summary}
\endgroup

Expertise leads to superior performance in specific domains, such as chess. Previous neuroimaging studies have investigated the extent to which such enhanced learning leads to specific functional activations in the brain. These studies however focused on the category level of expertise (i.e., chess versus nonchess stimuli). This poses limitations to our understanding of how expertise changes the brain. The recent development of multivariate statistical methods allows us to circumvent the issues posed by previous research and further elaborate on object representations' properties within a novel theoretical framework.

\vspace{0.2 cm}

For this thesis, we designed a comprehensive study to investigate the representational similarity among the objects of expertise at the subordinate level. This involved a behavioral pilot study and a multivariate fMRI study. We first developed a pilot study to validate our stimuli. This consisted of an online experiment in which intermediate and expert chess players solved the same chess positions. We compared both groups’ overall performance differences as well as consistency. Moreover, we also analyzed the influence of the level of difficulty posed by different questions on the performance of our two groups. For the second part of this thesis, we designed an fMRI experiment with innovative stimulus-rich event-related paradigms to compare the neural representational spaces of experts and novices. To explore the locations where the optimized neural representations are developed, an optimal experimental design is needed. Thus far, the initial pilot sessions point us toward a promising experiment.

\vspace{0.2 cm}

The behavioral study provided evidence with multiple statistical analyses that experts outperformed intermediates in the task. Moreover, we also supported our hypothesis related to the level of difficulty posed by each question. This allowed us to further select the stimuli for the fMRI study. We were able to design our fMRI study and test it with four pilot participants. The present thesis reports the results obtained for one participant with an intermediate chess rating, which suggests that our experimental task will produce informative findings. Further data collection is undergoing.

\newpage

\pagenumbering{roman}
\setcounter{page}{2}

\sectionfont{\centering}
\begingroup
\Large
\bfseries
\section*{Approach and Own Contribution}
\addcontentsline{toc}{section}{Approach and Own Contribution}
\endgroup

\textbf{Personal contribution.} The rationale for this study was proposed by my supervisor, Prof. Hans Op De Beeck, and initial co-supervisor, Dr. Artem Platonov. I contributed with the following parts:

\textit{Literature review.} At the beginning of this project, Dr. Artem Platonov shared some articles to highlight the theoretical and methodological aspects for this thesis. Based on these initial suggestions, I extended the literature review and identified relevant aspects for the study.

\textit{Stimuli development.} With guidance from Prof. Merim Bilalić, I envisioned (based on real chess games) and built all the chess stimuli. This consisted of researching for specific chess motifs, looking for games in chess databases, building the stimuli (checkmate, non-checkmate, and switched-sides versions), and writing a document with the explanations behind each position (including, the Forsyth-Edwards Notation (FEN) for each position, which supports computational reproducibility).

\textit{Programming the online experiment.} Independently, I used PsychoPy builder and Python to program the behavioral experiment. After this, I uploaded it on Pavlovia and monitored the results. Prof. Merim Bilalić and Dr. Artem Platonov provided feedback to improve some aspects of the experiment. One disclaimer: most of it could be done with PsychoPy Builder, but some specific features were not found in the Builder environment and were manually changed in the Python code provided by the PsychoPy Builder.

\textit{Collection of behavioral data and neural data.} Independently, I contacted chess players and collected behavioral data online. For the collection of neural data, Dr. Andrea Costantino and Laura van Hove were responsible for the fMRI technicalities, but I helped with recruitment during the Open Chess Leuven Tournament and other occasions, as well as with the pilot scanning sessions. For the scanning sessions, only four sessions were accomplished until now, and I participated in three. 

\textit{Part of the preprocessing.} Dr. Andrea Costantino and Laura Van Hove were responsible for this part (fMRIPrep and self-written MATLAB scripts). They shared all the information and scripts so that I could understand the process. Also, Laura Van Hove shared the files needed for conducting some of the latter aspects in SPM12 to compare it with the results obtained. This last part did not influence the results already obtained; it was just for knowledge \textit{per se}.

\textit{Analyses of the behavioral data.} Independently, I analyzed the behavioral data in R and JASP. All the data wrangling and code was written by me. The choices made in the analysis were based on my own insights and research. 

\textit{Analyses of the neural data.} This was the work of Dr. Andrea Costantino and Laura Van Hove. Again, they shared all the materials and explanations needed. Laura Van Hove and I went over the MATLAB scripts for conducting the MVPA and obtaining the dissimilarity matrix, which I later used "independently" to get Figure 10A.

\textit{Reporting.} With my supervisors’ feedback, I wrote this thesis. In order to support Open Science, I did everything in R Markdown. 

\newpage

\pagenumbering{roman}
\setcounter{page}{4}

\sectionfont{\centering}
\begingroup
\Large
\bfseries
\section*{Acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}
\endgroup

I would like to thank the ones directly associated with this thesis. My supervisor, Prof. Hans Op de Beeck, for welcoming me into this project and for all the support throughout it. It makes me happy to know that this project worked out, so far, as you envisioned. It is ironic that you study expertise when you are the expert yourself. Prof. Merim Bilalić for all the wisdom with the stimuli and feedback for the online experiment. I would say that my only disappointment was not being able to play a chess match with you. Dr. Artem Platonov for the support with the literature, feedback for the online experiment, and for allowing me to seek personal interests during this thesis. Dr. Andrea Costantino, mainly responsible for the neuroimaging part, I am grateful for all your work and the shared materials. You reignited the enthusiasm that I had lost for it. The Schaackclub Leuven for allowing me to be a member and for helping with the participants’ recruitment. The Brussels Chess Club for helping with the recruitment as well. Last, there is Laura Van Hove. Laura, if anyone ever praises me for this thesis once, you deserve to be praised twice. You are responsible for most of the knowledge I acquired about brain imaging data analysis. It is amazing to find people that are kind and knowledgeable, it was amazing to find you during this project.

\newpage

\pagenumbering{roman}
\setcounter{page}{5}
\addcontentsline{toc}{section}{Contents}
\tableofcontents

\newpage

\pagenumbering{roman}
\setcounter{page}{6}
\listoffigures

\newpage

\pagenumbering{roman}
\setcounter{page}{7}
\listoftables

\newpage

\pagenumbering{arabic}
\sectionfont{\centering}
\begingroup
\Large
\bfseries
\section*{Introduction}
\addcontentsline{toc}{section}{Introduction}
\endgroup

$\hspace{1cm}$ Human information processing is an incredibly complex ability. It refers to how our brains receive, store, and use information from the world. This ability allows us to identify and understand the objects in our environment. In fact, to orient ourselves in the world, optimal recognition of objects and their interconnection with the surroundings is essential [@hummel2013objectrecognition]. This crucial process enables us to learn and interact with the environment efficiently, such as recognizing and differentiating similar objects and faces [@hagentanaka2019; @jenkins2018]. The neurocognitive framework of visual perception suggests that there must be an internal network of neurons that store information as representations\footnote{The primary theoretical foundation in many areas of fMRI research is the notion that stimulus representations can be conceptualized as points within an n-dimensional space.} of visual input [@davispoldrack2013; @kamitanitong2005]. Building these functional representations of the world is only possible due to our learning capacity, a skill that needs to be better understood. The role of learning is evident when analyzing domains in which only a few individuals developed outstanding performance. Indeed, only a small percentage of people obtain high-level performance (i.e., expertise) in chess, for example. It is well-established that visual expertise alters the activation of objects of expertise in the brain [@bartlett2013expertise; @Bilalic2011; @bilalic2016faces; @floyd2017decoding; @rennig2013temporoparietal]. However, previous studies lack the necessary level of detail to fully explain the extent to which the representation of individual objects of expertise changes and how these changes unfold across the information processing hierarchy. This thesis constitutes an interdisciplinary approach - combining cognitive neuroscience and a computational paradigm - that aims to identify and comprehend better these changes that occur in the brain of chess experts.

\textbf{Object recognition and learning}

$\hspace{1cm}$ Past research has investigated a crucial aspect of human information processing: object recognition [@wernerchalupa2014; @gauthiertarr2016]. Although seemingly a non-demanding skill in everyday cognition, object vision entails some central problems, such as \textit{invariance} and \textit{category selectivity} [@dicarlo2007untangling; @gauthiertarr2016]. Invariance is related to the fact that we can recognize an object or scene as unchanged, despite variations in factors such as view, size, lighting, and arrangement, whereas complex selectivity is concerned with highly-selectivity of visual representations for a precise categorization and differentiation of exemplars (i.e., a dog is distinguished from a cat). Recent findings suggest that this seemingly unsolved dialogue between both above-mentioned features is mediated by the hierarchical processing in the visual cortex [@cohen2020separability], which entails that many different brain areas are involved in visual information processing [@duyck2023visualexpertise].

$\hspace{1cm}$ Hierarchical processing in the visual cortex can be better understood through the concept of \textit{object manifold} (see Figure 1). It can be thought of as a low-dimensional surface of data points in a high-dimensional space that portrays all possible variations of a single object (i.e., representational geometry) [@chungabbott2021]. That said, it is not necessary for all instances of an object to converge at a single point in this space. Instead, these instances can form a subspace or manifold that is distinguishable or "untangled" from other objects in that same space (for a more detailed conceptual clarification, see Chung \& Abbott, 2021), but this disentanglement happens as the information processing moves away from the retinal representational space [@dicarlo2007untangling]. In the visual system, the information acquired by the retina undergoes further processing stages, in which different features are extracted at each layer [@tenenbaum2000global]. Therefore, the output of certain layers provides input for the higher-order layers. The manifold helps to untangle the low-dimensional representation of one layer compared to the other [@dicarlo2012how]. This allows the brain to recognize objects despite variations in their appearance and multiple features, as it can represent the relevant information about each stimulus in a robust manner. Furthermore, following this line, it is expected that the brain develops complex representations of different stimuli based on multiple features. 

```{r object manifold, include = FALSE}
library(plotly) #needed for the plots

set.seed(42)

#generating entangled data
entangled_data1 <- data.frame(x = rnorm(100), y = rnorm(100), z = rnorm(100))
entangled_data2 <- data.frame(x = entangled_data1$x + rnorm(100, sd = 0.5),
                              y = entangled_data1$y + rnorm(100, sd = 0.5),
                              z = entangled_data1$z + rnorm(100, sd = 0.5))

#generating less entangled data
less_entangled_data1 <- data.frame(x = rnorm(100), y = rnorm(100), z = rnorm(100))
less_entangled_data2 <- data.frame(x = less_entangled_data1$x + rnorm(100, mean = 4, sd = 0.5),
                                   y = less_entangled_data1$y + rnorm(100, mean = 4, sd = 0.5),
                                   z = less_entangled_data1$z + rnorm(100, mean = 4, sd = 0.5))

#the parameters here where chosen arbitrarily. what matters for this context is just that they overlap in the former, while in the latter they dont

#entangled 3D scatter plot
entangled_plot <- plot_ly() %>%
  add_trace(data = entangled_data1, x = ~x, y = ~y, z = ~z, type = "scatter3d", mode = "markers", marker = list(color = "red", size = 3)) %>%
  add_trace(data = entangled_data2, x = ~x, y = ~y, z = ~z, type = "scatter3d", mode = "markers", marker = list(color = "blue", size = 3)) %>%
  layout(title = "Entangled Distributions", scene = list(xaxis = list(title = "X"), yaxis = list(title = "Y"), zaxis = list(title = "Z")))

#less entangled 3D scatter plot
less_entangled_plot <- plot_ly() %>%
  add_trace(data = less_entangled_data1, x = ~x, y = ~y, z = ~z, type = "scatter3d", mode = "markers", marker = list(color = "red", size = 3)) %>%
  add_trace(data = less_entangled_data2, x = ~x, y = ~y, z = ~z, type = "scatter3d", mode = "markers", marker = list(color = "blue", size = 3)) %>%
  layout(title = "Less Entangled Distributions", scene = list(xaxis = list(title = "X"), yaxis = list(title = "Y"), zaxis = list(title = "Z")))

entangled_plot
less_entangled_plot
```

\begin{figure}
\centering
\makebox[\textwidth][c]{\includegraphics[width=0.8\textwidth]{object manifold.png}}
\captionsetup{labelformat=empty, font=scriptsize, justification=raggedright}
\caption[\textbf{\textit{Figure 1.}} Illustration of neural manifolds.]{\textbf{\textit{Figure 1.} Illustration of neural manifolds.} (a) Neural manifolds arise as a result of stimulus variability. Population responses to two object classes (dog vs. cat) in the presence of the stimulus variability (orientation) gives rise to two object manifolds. Invariant object recognition becomes the problem of classifying between two object manifolds. Axes represent the firing rates of neurons. (b) Manifold capacity is high if object manifolds are well separated and low when object manifolds are entangled in neural state space. Figure and caption reprinted with permission from Chung \& Abbott, 2021, p. 139.}
\end{figure}

$\hspace{1cm}$ Previous studies have highlighted the role of brain plasticity and learning for object representations. Object recognition is susceptible to enhancement by visual perceptual learning (VPL), which is a prolonged enhancement in the visual system’s capacity to recognize and differentiate stimuli achieved with practice [@awada2021visual]. Connecting this to the manifold framework, one may argue that learning leads to an optimization in the disentanglement of the manifolds of individual objects due to its increase in visual selectivity. Early studies addressing visual expertise have shown that experts exhibit increased neural activation for the expertise-object category compared to novices. This was, for example, found in visual regions such as the fusiform face area (FFA) [@harley2009engagement] and lateral occipital complex (LOC) [@opdebeeck2006discrimination], and even in prefrontal regions [@krawczyk2011neural]. 

$\hspace{1cm}$ Most of these studies, however, focused on the neural activation underlying expert object categorization. Experts demonstrate superior performance in the classification and identification of visually similar objects at a subordinate level\footnote{An expert is able to identify that a specific position was achieved through the Ruy López Opening: Main line instead of Ruy López Opening: Exchange Variation, whereas a beginner will not be able to discriminate at this fine level. In fact, some chess experts can know up to 36 moves in the Ruy López Opening: Marshall Attack, Modern, Main Line, Spassky Variation.} (i.e., a chess expert identifies differences between different chess openings, whereas a novices will only know that it is an opening) [@duyck2021visual; @tanaka1991object]. This lack of fine-grained distinctions is possible to overcome due to the development of multivariate statistical methods and designs that provide tools for analyzing and modelling complex, multi-dimensional data, making it possible to study the relationships between different features of a stimulus and their influence on visual processing [@opdebeeck2019introduction]. Therefore, recent studies moved beyond the overall brain activation in different regions to investigating differences between closely localized voxels.

\textbf{Multivariate statistical techniques}

$\hspace{1cm}$ @haxby2001distributed showed that the ventral temporal cortex has distinct response patterns for different object categories, including man-made objects, which are not limited to categories with dedicated anatomical regions. This study was the first to use multi-voxel pattern analyses (MVPA) [@haxby2012multivariate]. In the context of expertise, Martens et al. (2018) found that expertise influences the neural object representations in high-level visual areas. This technique, in contrast to voxel-wise analyses, searches for differences between voxels instead of assuming that nearby voxels show similar activation [@mahmoudi2012multivoxel; @opdebeeck2019introduction].

$\hspace{1cm}$ Researchers have also made use of another method called representation similarity analyses (RSA), which further addresses other limitations of univariate techniques, such as the loss of information due to signal averaging across voxels and how information is represented in a high-order space [@popal2019guide]. This allows researchers to compare representations of certain stimuli across different dimensions. For instance, Bracci and Op de Beeck (2016) indicated that shape and category representations coexist independently but are also intimately related throughout the visual hierarchy. Notwithstanding, in the context of visual expertise, Duyck et al. (2021) showed that the representations at the neural level between birds experts and participants with little birds knowledge are different, especially in the frontal cortex (FC) and high-level visual cortex (HVC). This study precisely illustrates the extent in which expertise might change neural information processing.

$\hspace{1cm}$ Therefore, MVPA decoding offers a more in-depth understanding of which areas contribute to a process and provides insight into how these areas differentiate information. RSA increases the captured knowledge by directly comparing individual responses. By doing so, RSA provides information on how different brain regions represent information. This technique creates a complete picture of the structure of information representation across a higher-order dimensional space [@davispoldrack2013; @kriegeskorte2008representational; @popal2019guide]. Importantly, RSA also allows us to develop comparisons that are not restricted to neural and behavioral data, such as units from an artificial neural network [@kubilius2016deep; @opdebeeck2019introduction], which fosters the implementation of innovative stimulus-rich event-related paradigms [@bracci2016dissociations]. Thus, these methods probe the opportunity to bridge the behavioral and neural representational geometry in the domain of chess expertise. 

\textbf{Expertise in chess}

$\hspace{1cm}$ The study of expertise in board games, specifically chess, is responsible for igniting the research on expertise as a broader domain (Bilalić, 2017; Gobet, 2018). Indeed, chess is the expertise domain with the longest tradition in cognitive and computer science [@Binet1966; @deGroot1978; @ChaseSimon1973]. Possible explanations for this popularity can be traced to the fact that the needed cognitive processes for it resemble real-life processes and the ease of obtaining ecological validity in the experimental scenario (i.e., the tasks represent well the common environment of chess players) [@Bilalic2012; @Bilalic2007; @Gobet2004]. 

$\hspace{1cm}$ Given its intrinsic characteristics (i.e., complicated spatial and functional relationship between objects), complex cognitive processes, such as object and pattern recognition, are required to develop expertise fully. Research has shown that chess positions are often categorized by experienced players based on piece relationships or abstract similarities, rather than visual characteristics that impact the perception of less experienced players [@BilalicGobet2009; @LeonVillagra2013; @LinharesBrum2007]. This difference, curiously, disappears in random chess positions [@Bilalic2010]. That said, some theories — chunking theory (CT) and template theory (TT) (for a review, see Bilalić, 2017; Gobet, 2018) — laid out principles that are able to explain experts’ performance. The TT proposes, in addition to chunks (i.e., meaningful units of individual objects), that templates are formed through the frequent use of chunks. In that way, it provides mechanisms that explain how the high-level memory structures — templates — are generated from smaller memory units — chunks [@Gobet2018]. Thus, templates are more dynamic and flexible patterns. Despite the differences, there is a fundamental agreement: the pattern recognition process in long-term memory enables efficient information intake, which automatically triggers the recall of stored knowledge associated with the recognized situation at hand. This memory-attention loop leads to a biased attention toward relevant aspects of the current situation (Bilalić, 2017). But how does the brain implement problem-solving strategies supported by this knowledge? 

\textbf{Neural basis of chess expertise}

$\hspace{1cm}$ Past research has shown multiple brain regions involved with chess activities. The FFA (Bartlett et al., 2013; Bilalić et al. 2011; Krawczyk et al., 2011), the temporal parietal junction (TPJ) (Rennig et al., 2013), the posterior cingulate cortex (PCC) (Bartlett et al., 2013; Krawczyk et al., 2011), the parahippocampal place area (PPA) (Bilalić et al., 2010, 2012), the posterior middle temporal gyrus (pMTG), and the left supramarginal gyrus (SMG) (Bilalić et al., 2011) all have been associated with chess to some extent, as well as other areas. Interestingly, these studies also show a crucial aspect of visual object recognition, which is the perception of individual entities and their relations. This entails the focus on two different aspects: \textit{skilled object recognition} and \textit{skilled pattern recognition}. 

$\hspace{1cm}$ @Bilalic2017 demonstrated that experts and novices both showed increased activation in the pMTG and surrounding areas when identifying chess pieces. However, experts activated both pMTGs, while novices only activated the pMTG in the left hemisphere. With another task, where retrieval of the object's function was required, experts showed increased activation in the pMTG and left SMG, while novices did not exhibit activation in the SMG. This suggests that the pMTG and SMG are essential for fast and efficient perception of objects and their functions. 

$\hspace{1cm}$ Moreover, in other studies that required a more holistic view of the chessboard, different brain regions were associated with it. When having to localize objects and threats in chess, experts and novices showed activation in several lateral brain areas. Experts exhibited greater activation in the right pMTG and the left SMG. Both groups also demonstrated activation in several areas on the medial side, including the parahippocampal gyrus (PHG), the retrosplenial cortex (RSC), the precuneus (PCun), and the caudate nucleus (CdN). In another task, where participants had to differentiate between normal and random positions, experts showed increased activation in the right superior colliculus (RSC) and the PHG [@Bilalic2010; @Bilalic2012]. The authors concluded that the PHG/PPA and RSC are involved in the processing of patterns among the pieces, a finding that was later conceptually replicated [@bartlett2013expertise].

$\hspace{1cm}$ Despite the recent findings, it remains unclear what representational changes undermine the modulation of the human brain, a domain-general system, to acquire such expertise in a tailored domain. Precisely because the past research has focused on the neural activation for the expertise category (i.e., superordinate and basic level) instead of the neural activation for subordinate level, which is possible to investigate due to the techniques such as MVPA and RSA.

\textbf{The current study}

$\hspace{1cm}$ In this study, we will present a set of chess positions in which relevant dimensions can be dissociated. The chess visualization was developed in order to differentiate significant perceptual dimensions, which may prevail in novices (i.e., visual resemblance) and experts (i.e., chunks and templates). In a condition-rich fMRI design, experts and novices will evaluate chess positions, which will present various board configurations. Our initial analysis will focus on the representational similarity among the test images, expecting to uncover basic visual dimensions. Furthermore, we anticipate that expert representations will provide additional insights into features related to the interrelations between pieces and accurate position evaluation. This analysis offers the intriguing prospect of observing the development of these representations throughout the processing hierarchy. In line with the idea that experts will demonstrate superior linear separability in distinguishing critical differences between the positions.

$\hspace{1cm}$ This present study extends the literature in multiple ways. As mentioned, previous neuroimaging research lacks neural data at a detailed level — i.e., only evaluating functional specificity for the entire domain of expertise [@Campitelli2007brain; @Hanggi2014architecture; @Premi2020enhanced; @Song2020altered] and even morphometric changes [@RaviPrakash2021morphometric]. First, we use MVPA fMRI to investigate the extent to which visual expertise changes the \textit{representation} of the objects of expertise in the brain. Second, a classic hypothesis in the learning literature is that training leads to optimized representations that prioritize relevant dimensions over irrelevant dimensions [@Goldstone1998perceptual; @Kellman2013perceptual]. However, the shift towards a manifold perspective suggests that a more linearly separable representation does not mean discarding irrelevant dimensions. @Hong2016explicit found that low-level features like object position are often represented more effectively, not less effectively, in higher processing levels in both primate brains and deep convolutional neural networks (DCNNs). Thus, a revised hypothesis is that acquiring expertise results in optimized manifold representations that make important stimulus distinctions more linearly separable while still preserving sensitivity to other dimensions. 

$\hspace{1cm}$ In order to achieve this, we first conducted a behavioral pilot study to validate our stimuli. This part consisted of an online experiment in which chess players, with different ability levels, had to evaluate chess positions (Section I). Once this was accomplished, fMRI sessions were planned so that we could further apply the aforementioned methods (Section II). For this thesis, the acquisition of neural data was not fully performed, but we were able to implement and test it with four pilot participants. 

\sectionfont{\centering}
\begingroup
\Large
\bfseries
\section*{Section I: Behavioral pilot study}
\addcontentsline{toc}{section}{Section I: Behavioral pilot study}
\endgroup

\subsection*{Main objectives}
\addcontentsline{toc}{subsection}{Main objectives}

$\hspace{1cm}$ The pilot study aimed to test and validate the stimuli. The chessboard positions were developed in a specific manner so that we could distinguish between two dimensions: visual and abstract similarity. In other words, the stimuli were designed to contain low-level (visual similarity) and high-level (semantic information) features. For this part of the study, we were not interested in dissociating these dimensions. We simply wanted to investigate whether the stimuli would differentiate experts and novices at the behavioral level in terms of performance. This study was designed to discern any stimuli that underperformed relative to the others, especially considering the variance in difficulty associated with the different positions (i.e., easy and hard questions).

$\hspace{1cm}$ Therefore, we hypothesized that experts would have a higher amount of correct responses in comparison to intermediates (H1). We also hypothesized that experts would have lower response times in comparison to intermediates (H2). In line with this, we hypothesized that experts would be a more consistent group than intermediates (H3). Lastly, we also expected that in easy positions the performance between both groups would be similar (H4). 

\subsection*{Methods}
\addcontentsline{toc}{subsection}{Methods}

```{r power, include = FALSE}
library(stats)
power.t.test(n = NULL, d = 0.7, sig.level = 0.05, power = 0.8, type = "two.sample", alternative = "one.sided")
```

$\hspace{1cm}$ \textbf{\textit{Participants.}} For the behavioral study, an a priori power analysis was performed in R 4.1.2 [@RCoreTeam2021] solely for a two-sample \textit{t}-test to determine the required sample size to obtain a power of 0.8, given the assumption of detecting, at least, a medium-to-large effect size [@lakens2022]. For this, an effect size of 0.7, a significance level of 0.05, and a one-sided alternative hypothesis were assumed. The analysis indicated a sample size of approximately 25 participants per group. 

$\hspace{1cm}$ Participants were recruited initially through Lichess and Chess.com — both leading online platforms for chess players. Later, we extended also to Facebook chess groups. During the recruitment, more than 80 chess players from different levels were individually contacted, but only 33 participants agreed to participate and, out of those, 19 participants completed the study ($M_{age}$ = 30 [SD = 8.8], 13 males). Of these participants, 10 had an official rating above 1800 Elo points (see Supplementary Materials for a list). The Elo rating system consists of an interval scale with a theoretical mean of 1500 and a standard deviation of 200 [@Elo1978rating; @vandermaas2005]. Usually, chess experts are defined as players with a rating of 2000 Elo points or more. The expert sample corresponds closely to the sample size in previous behavioral research on expertise [@Bilalic2012; @brockmole2008role; @kiesel2009playing]. The other 9 participants were intermediates. At first, we planned on strictly recruiting novices as defined in past experiments: individuals who had played chess before and knew the rules, but were not active and regular players (Bilalić, 2010). However, the online ratings of the participants were more in line with an intermediate category.

$\hspace{1cm}$ The procedures described here were approved by the ethical committee of the Catholic University of Leuven and all participants for the behavioral study provided informed consent before starting the online experiment.

$\hspace{1cm}$ \textbf{\textit{Stimuli.}} The stimulus set consisted of 26 full-board chess positions. The positions were constructed based on existing games to maintain fidelity to natural chess positions. Most positions had a possible checkmate within either three or four moves, except the so-called easy positions in which there was a checkmate in one move (i.e., 3, 6, 12, 15, 22, and 24). Moreover, as there were meaningful differences between positions, but also significant similarities between others (i.e., pieces involved and alike motifs), we arguably categorized the positions in five different categories. For each of the 26 positions we had a visually similar, but meaningfully different position in which a forced checkmate was not possible (see Figure 2). These paired positions are important for the fMRI design, but they were not included in the behavioral pilot study (see Appendix for the complete stimuli set and respective checkmate explanations).

\begin{figure}
\centering
\makebox[\textwidth][c]{\includegraphics[width=0.8\textwidth]{Exampleformethodspositionwithbar.png}}
\captionsetup{labelformat=empty, font=scriptsize, justification=raggedright}
\caption[\textbf{\textit{Figure 2.}} Comparison between a position with checkmate and visually similar position without checkmate.]{\textbf{\textit{Figure 2.} Comparison between a position with checkmate and visually similar position without checkmate.} The chessboard on the left leads to a forced checkmate after Ng5+ or Bg8+ is played by white. However, if the pawn on h6 is moved to h5, there is no forced checkmate anymore as the evaluation bar shows the advantage for black instead (Stockfish 11, search-depth 20). The M4 evaluation for white means that there is a possible checkmate in 4 moves, whereas the 8.6 evaluation for black highlights an advantage in terms of material and positional aspects (one possible interpretation: black is up by 8.6 pawns).}
\end{figure}

$\hspace{1cm}$ To guarantee an absence of cross-cultural issues related to the methodology applied [@Gobet2004] and other relevant stimuli-artefacts that could interfere with the fMRI study, two rated chess players were responsible for the stimuli development: the main author of this thesis, an intermediate player, and a FIDE chess master. Merim Bilalić, with a standard format peak of 2308 Elo rating, ensured the quality of the positions. 

$\hspace{1cm}$ \textbf{\textit{Task and procedure.}} Participants were asked to analyze 26 randomly presented chess positions. The image set was sized to be displayed at half the width and half the height of the screen. The participants received the following instruction: \textit{“For each image you have to ask yourself if there is a forced checkmate. If there is a checkmate, press your left arrow key. After that, a new screen will appear. Please, type the first move for white that leads to checkmate”}. This last requirement was responsible for preventing hectic answers. In other words, we would be able to tell if the participant indeed found the checkmate solution instead of randomly pressing the available keys (left for \textit{yes} or right for \textit{no}). There were no instructions concerning a time limit for their answers to be given. The experiment was made available online with the platform Pavlovia and had a compensation of 10 euros.

$\hspace{1cm}$ \textbf{\textit{Statistical analyses.}} The behavioral analyses were divided into three sets. We initiated with non-parametric tests to analyze how both groups performed differently in the task. Following this, we were interested in computing how the consistency between both groups differed. Lastly, we delved into the relevance of each question (chess positions) for the performance of both groups.

$\hspace{1cm}$ To investigate the differences in performance, a one-sided permutation Wilcoxon-Mann-Whitney (WMW) test (also referred to as Mann-Whitney U test or the Wilcoxon rank sum test) was conducted. This test accounts for small sample sizes and demands less restrictive assumptions than a basic WMW test [@Divine2018wilcoxon; @fay2010wilcoxon]. In this case, the Mann-Whitney effect parameter ($\Phi$) represents the estimated probability that a randomly chosen observation from one group has a lower (or higher) value than a randomly chosen observation from the other group, plus half the probability that they are equal (1). This entails that an effect parameter $\Phi$ of 0.5 suggests no difference between the two groups, whereas anything below or above implies a difference [@fay2018confidence]. For this initial set of analyses, we compared both groups in terms of their correct answers (H1: Experts > Intermediates) and response times (H1: Experts < Intermediates).

\begin{align}
\phi = \Pr(X < Y) + 0.5 \cdot \Pr(X = Y)
\end{align}

$\hspace{1cm}$ A Bayesian alternative was also implemented to test the same hypotheses in JASP version 0.17.2 [@jasp2023]. This approach was based on the outlined procedure in @vanDoorn2020bayesian, in which the parameter $\delta$ represents the difference in location parameters between the two groups in their respective latent normal distributions. The hypotheses specified were H0: $\delta$ = 0 against H1: $\delta$ > 0 for the total score and H0: $\delta$ = 0 against H1: $\delta$ < 0 for the response times comparison. The null hypothesis posits that there is no difference between the groups, whereas the alternative posits a directional-difference and assigns the parameter ($\delta$) a prior Cauchy distribution with scale parameter set to 1/$\sqrt{2}$ (2). To compare the relative evidence for the null and alternative hypotheses given our data, we calculated the Bayes factor (see Doorn et al., 2020 for a detailed explanation).

\begin{align}
\delta \sim \text{Cauchy}(0, \frac{1}{\sqrt{2}})
\end{align}

$\hspace{1cm}$ Two different analyses were conducted for the consistency measure: (i) comparison between the within- and between-group dissimilarities and (ii) multidimensional scaling (MDS). First, we selected and standardized the response times (in minutes) and the total correct answers of each participant. With the standardized data, we calculated a distance matrix (i.e., euclidean distance), meaning that we measured the distance between all pairs of participant-specific observations (see (3) for an example of the distance measure between two participants). This resulted in dissimilarity values between all participants.

\begin{align}
d(P, Q) = \sqrt{(x2 - x1)^2 + (y2 - y1)^2}
\end{align}

where $P (x1, y1)$ represents the standardized response time and total correct answers for one participant, whereas $Q (x2, y2)$ represents the same variables for another participant. 

$\hspace{1cm}$ That said, we acquired a 19x19 matrix in which every individual was compared to the other remaining participants. Using only the values above the main diagonal to avoid redundant information (i.e., identical comparison values and diagonal values), we calculated the within- and between-group dissimilarities. Each cell in the matrix represented the dissimilarity between the individual of a given row and column. The within-group dissimilarity was obtained from cells involving only experts (or intermediates) in both rows and columns. In other words, we selected the dissimilarity values among individuals that belonged to the same group and calculated the mean for the group. Whereas the between-group dissimilarity was derived from cells involving an expert and an intermediate. This means that we selected only the dissimilarity values between experts and intermediates, representing the dissimilarities between the participants in these two groups, and calculated the average.

$\hspace{1cm}$ We also performed a MDS — referred as principal coordinates analysis (PCoA) [@gower1966] — to investigate the relative \textit{proximities} among participants in a low-dimensional space [@Mair2018]. This analysis allows us to inspect the structure of the dissimilarity. The procedure was done with the values above the main diagonal from the same abovementioned distance matrix, which means that we compared the participants in terms of their standardized response times and total correct answers.

$\hspace{1cm}$ The last set of analyses were tailored to investigate the questions’ influence on the performance of participants. In a descriptive manner, we calculated the percentage of correct answers for both groups in each question. We elaborated on the same findings with a mixed-effects logistic regression model (4) to compute the predicted probabilities and odds ratio. We estimated the probability of a correct answer as a function of two predictors: the group level (i.e., intermediates and experts) and difficulty level of the question (i.e., easy and hard). Hence, we clustered all the easy and hard questions into one variable with two categories. We accounted for the observation-dependency (i.e., same participant answering multiple questions) with the random intercept for each participant. Lastly, we computed a likelihood ratio test (LRT) to evaluate the significance of the \textit{difficulty level} predictor. Here we compared two models: model 1, which included only the group variable, was compared to model 2, which included both group and difficulty level variables. The residual diagnostics were verified with \textit{DHARMa} version 0.4.6 [@dharmaR], a R package that uses a simulation-based approach to assess model fit.

\begin{equation}
\text{logit}(p_{ij}) = \beta_{0} + \beta_{1}X_{\textit{Difficulty level}{ij}} + \beta{2}X_{\textit{Group}{ij}} + b0_{j} + \epsilon_{ij}
\end{equation}

where $b0_{j}$ is the random intercept for participant $j$.

$\hspace{1cm}$ A Bayesian mixed-effects logistic regression was also conducted (see Supplementary Materials) to make our results more reliable. Previous research highlighted that Bayesian methods are suitable to reduce model misspecification and estimation bias in cases with small sample sizes [@ali2019sample].

$\hspace{1cm}$ \textbf{\textit{Materials.}} This thesis was written with all the possible efforts (and knowledge at disposal) to support reproducibility and Open Science. The manuscript was written using R Markdown and the available repository (https://github.com/felipelfv/KULeuven_Thesismaterials) contains all the material used for section I as well as a \textit{sessionInfo} document that highlights the packages used for the behavioral analyses.

```{r data wrangling, echo=FALSE, results='hide'}
data <- read_excel("~/Desktop/missingpartincluded.xlsx")

#rename trials.thisindex to questions going from 1-26 instead of 0-25
data <- data %>%
  group_by(participant) %>%
  mutate(questions = c(1:26)) %>%
  ungroup()

#mutate trials.thisindex, participant, answer and questions
data <- data %>%
  mutate(trials.thisIndex = as.factor(trials.thisIndex)) %>%
  mutate(participant = as.factor(participant)) %>%
  mutate(Answer = as.factor(Answer)) %>%
  mutate(questions = as.factor(questions))
#summary(data)

#recode score
data <- mutate(data, Answer = dplyr::recode(Answer,"right" = 1, "wrong" = 0))

#summary per participant
sumstatsbyparticipant <- data %>%
  group_by(participant) %>%
  summarise(amount_correct = sum(Answer),
            score = mean(Answer), # score for that participant (i.e. mean across the questions for that participant)
            sd = sd(Answer),
            min = score - sd(Answer)/ sqrt(n()), #meanscore for that participant - 1 standard error 
            max = score + sd(Answer)/ sqrt(n()), #meanscore + 1 standard error
            key_resp_rt_minutes = mean(key_resp.rt / 60)) #check this; before it worked without it. converting to min

#adding in participants group (ranked as 1 vs not rankedas 0)
sumstatsbyparticipant <- sumstatsbyparticipant %>%
  mutate(rankings = c(0,1,0,1,1,0,0,1,1,1,0,0,0,1,0,1,0,1,1)) %>%
  mutate(rankings = as.factor(rankings)) %>%
  mutate(total_rptime = key_resp_rt_minutes * 26)

rankingdat <-sumstatsbyparticipant %>%
  select(participant, rankings)

#adding rankings to data 
data <- inner_join(x = data, y = rankingdat, by = "participant")
#str(data)

#ranked players 
rankedplayers <- data %>%
  filter(rankings == "1") %>%
  group_by(questions) %>%
  summarise(amount_correct = sum(Answer),
            scoreR = mean(Answer),
            sd = sd(Answer),
            min = scoreR - sd(Answer)/ sqrt(n()), #meanscore - 1 standard error (SE = sd/sqrt(26), gives us the standard error)
            max = scoreR + sd(Answer)/ sqrt(n()), #meanscore + 1 standard error
            minimum = min(Answer),
            maximum = max(Answer),
            median = median(Answer))

rankedplayersrt <- data %>%
  filter(rankings == "1") %>%
  summarise(total_averagert = mean(key_resp.rt),
            total_sumrt = sum(key_resp.rt))

#nonranked players
nonrankedplayers <- data %>%
  filter(rankings == "0") %>%
  group_by(questions) %>%
  summarise(amount_correct = sum(Answer),
            scoreNR = mean(Answer),
            sd = sd(Answer),
            min = scoreNR - sd(Answer)/ sqrt(n()), #meanscore - 1 standard error (SE = sd/sqrt(26) gives us the standard error 
            max = scoreNR + sd(Answer)/ sqrt(n()), #meanscore + 1 standard error
            minimum = min(Answer),
            maximum = max(Answer),
            median = median(Answer),
            total_averagert = mean(key_resp.rt))

nonrankedplayersrt <- data %>%
  filter(rankings == "0") %>%
  summarise(total_averagert = mean(key_resp.rt),
            total_sumrt = sum(key_resp.rt))

#total number of correct answers for each participant
total_correct <- aggregate(Answer ~ participant, data = data, FUN = function(x) sum(x == 1))

#renaming the new column to "total_correct"
colnames(total_correct)[2] <- "total_correct"

#total number of incorrect answers for each participant
total_incorrect <- aggregate(Answer ~ participant, data = data, FUN = function(x) sum(x == 0))

#renaming the new column to "total_incorrect"
colnames(total_incorrect)[2] <- "total_incorrect"

#merging the total correct and incorrect answers with the original dataset
data <- merge(data, total_correct, by = "participant")
data <- merge(data, total_incorrect, by = "participant")
```

\subsection*{Results}
\addcontentsline{toc}{subsection}{Results}

```{r assumptions, echo = FALSE, include = FALSE}
#normality
#based on score
normality <- sumstatsbyparticipant %>%
  group_by(rankings) %>%
  mutate(group_resid = score - mean(score)) %>%
  ungroup()

qqPlot(normality$group_resid)
qqPlot(sumstatsbyparticipant$score)
shapiro_score <- shapiro.test(sumstatsbyparticipant$score)

#normality
#based on rt
normality <- sumstatsbyparticipant %>%
  group_by(rankings) %>%
  mutate(group_resid = key_resp_rt_minutes - mean(key_resp_rt_minutes)) %>%
  ungroup()

qqPlot(normality$group_resid)
qqPlot(sumstatsbyparticipant$key_resp_rt_minutes)
shapiro_rt <- shapiro.test(sumstatsbyparticipant$key_resp_rt_minutes)

#relevant values
shapiro_score_W <- shapiro_score$statistic
shapiro_score_p <- shapiro_score$p.value

shapiro_rt_W <- shapiro_rt$statistic
shapiro_rt_p <- shapiro_rt$p.value

#equal variances
#Levene's test
leventestscore <- leveneTest(score ~ rankings, data = sumstatsbyparticipant)
leventestrt <- leveneTest(key_resp_rt_minutes ~ rankings, data = sumstatsbyparticipant)

#relevant values
levenscore_df <- leventestscore[1,1]
levenscore_dff <- leventestscore[2,1]
levenescore_F <- leventestscore$`F value`[-2]
levenescore_p <- leventestscore$`Pr(>F)`[-2]

#relevant values
levenrt_df <- leventestrt[1,1]
levenrt_dff <- leventestrt[2,1]
levenrt_F <- leventestrt$`F value`[-2]
levenrt_p <- leventestrt$`Pr(>F)`[-2]
```

```{r wilcoxon, echo = FALSE, include = FALSE}
#package
#install.packages("asht")
library(asht)
#The wmwTest function calculates the Wilcoxon-Mann-Whitney test (normal approximation, exact complete enumeration, and exact Mante Carlo implementation) 
#together with confidence intervals on the Mann-Whitney parameter, Pr[ X<Y] + 0.5 Pr[X=Y].
sumstatsbyparticipant$rankings <- relevel(sumstatsbyparticipant$rankings, ref = "0") #reference level used to compare the groups (i.e,, "group 1 is bigger/larger than the reference level")
m1 <- wmwTest(score ~ rankings, data=sumstatsbyparticipant, alternative=c("greater"))
m1
m2 <- wmwTest(key_resp_rt_minutes ~ rankings, data=sumstatsbyparticipant, alternative=c("less"))
m2

#multiple comparison are not strictly necessary with a set a of plausible small comparisons 
#plus the bayesian version implemented in JASP also adds to as a test without the need for multiple comparisons
#even if some bayesians argue that the prior is crucial for the choice of correction or not, such as M. Lee mentioned once
p.adjusted <- p.adjust(c(m1$p.value, m2$p.value), method = "bonferroni")
print(p.adjusted)
#bonferroni correction in this case
#the correcting does not affect the conclusion: 0.0009325234 0.0028804847

#relevant values
m1_W <- m1$statistic
m1_p <- m1$p.value
m1_CI <- m1$conf.int

m2_W <- m2$statistic
m2_p <- m2$p.value
m2_CI <- m2$conf.int

#NOT USED FOR THE THESIS MANUSCRIPT
#posthoc power is not a good choice (see Lakens, 2022, and others)
#this code was just to try out the results, but it was not used for any part of the thesis
#sample size for both groups
#n1 <- length(sumstatsbyparticipant$score)
#n2 <- length(sumstatsbyparticipant$key_resp_rt_minutes)
#power for m1
#pwr.r.test(n = n1, r = abs(effsize_m1$effsize), sig.level = 0.05, alternative = "two.sided")
#power for m2
#pwr.r.test(n = n2, r = abs(effsize_m2$effsize), sig.level = 0.05, alternative = "two.sided")
```

```{r effect size, echo = FALSE, include = FALSE}
#install.packages('effectsize')
library(effectsize)

#NOT USED FOR THE THESIS MANUSCRIPT
#create separate vectors for each group
group1_scores <- sumstatsbyparticipant$score[sumstatsbyparticipant$rankings == levels(sumstatsbyparticipant$rankings)[1]]
group2_scores <- sumstatsbyparticipant$score[sumstatsbyparticipant$rankings == levels(sumstatsbyparticipant$rankings)[2]]

group1_times <- sumstatsbyparticipant$key_resp_rt_minutes[sumstatsbyparticipant$rankings == levels(sumstatsbyparticipant$rankings)[1]]
group2_times <- sumstatsbyparticipant$key_resp_rt_minutes[sumstatsbyparticipant$rankings == levels(sumstatsbyparticipant$rankings)[2]]

#calculate rank biserial correlations
rank_biserial1 <- effectsize::rank_biserial(x = group2_scores, y = group1_scores)
rank_biserial2 <- effectsize::rank_biserial(x = group2_times, y = group1_times)

rhoscorevalue <- rank_biserial1$r_rank_biserial 
rhoscorecilow <- rank_biserial1$CI_low
rhoscoreciup <- rank_biserial1$CI_high

rhortvalue <- rank_biserial2$r_rank_biserial 
rhortcilow <- rank_biserial2$CI_low
rhortciup <- rank_biserial2$CI_high
#More about the rank biserial correlation used can be found in van Doorn et al., 2022 (p. 2988). Here I mention it briefly:
#The rank-biserial correlation can also be expressed as the difference between the proportion of data pairs where $x_i > y_j$ versus $x_i < y_j$:

#\begin{equation}
#\rho_{rb} = \sum_{i=1}^{n_1} \sum_{j=1}^{n_2} Q(x_i - y_j)
#\end{equation}
```

```{r averages, echo = FALSE, include = FALSE}
#grouping the data by the 'rankings' column and calculate the average and standard deviation
averages <- data %>%
  mutate(key_resp_rt_adjusted = (key_resp.rt/60)) %>%
  group_by(rankings) %>%
  summarise(
    average_key_resp_rt_adjusted = mean(key_resp_rt_adjusted, na.rm = TRUE),
    sd_key_resp_rt_adjusted = sd(key_resp_rt_adjusted, na.rm = TRUE),
    median_key_resp_rt_adjusted = median(key_resp_rt_adjusted, na.rm = TRUE),
    iqr_key_resp_rt_adjusted = IQR(key_resp_rt_adjusted, na.rm = TRUE),
    average_total_correct = mean(total_correct, na.rm = TRUE),
    sd_total_correct = sd(total_correct, na.rm = TRUE),
    median_total_correct = median(total_correct, na.rm = TRUE),
    iqr_total_correct = IQR(total_correct, na.rm = TRUE)
  )

#relevant values
#median and iqr for correct answers
MdnExperts <- averages$median_total_correct[2]
IQRExperts <- averages$iqr_total_correct[2]
MdnBeginners <- averages$median_total_correct[1]
IQRBeginners <- averages$iqr_total_correct[1]

#mean and sd for correct answers
MExperts <- averages$average_total_correct[2]
SDExperts <- averages$sd_total_correct[2]
MBeginners <- averages$average_total_correct[1]
SDBeginners <- averages$sd_total_correct[1]

#median and iqr for response time
MdnExpertsRT <- averages$median_key_resp_rt_adjusted[2]
IQRExpertsRT <- averages$iqr_key_resp_rt_adjusted[2]
MdnBeginnersRT <- averages$median_key_resp_rt_adjusted[1]
IQRBeginnersRT <- averages$iqr_key_resp_rt_adjusted[1]

#mean and sd for response time
MExpertsRT <- averages$average_key_resp_rt_adjusted[2]
SDExpertsRT <- averages$sd_key_resp_rt_adjusted[2]
MBeginnersRT <- averages$average_key_resp_rt_adjusted[1]
SDBeginnersRT <- averages$sd_key_resp_rt_adjusted[1]
```

$\hspace{1cm}$ The choice of a permutation WMW test for this study was primarily based on literature recommendation [@Divine2018wilcoxon; @Hart2001mannwhitney] and on the violation of the assumptions required for conducting a two-sample \textit{t}-test. We first conducted a Shapiro-Wilk test to assess the normality (\textit{W} = `r round(shapiro_score_W, 2)`, \textit{p} < 0.001) and a Levene's test to evaluate the homogeneity of variances (\textit{F}(`r levenscore_df`, `r levenscore_dff`) = `r round(levenescore_F,2)`, \textit{p} = `r round(levenescore_p,2)`) for the correct answers across the two groups. This same procedure was applied to the response times. The Shapiro-Wilk test, in this case, also revealed that the response times are not normally distributed (\textit{W} = `r round(shapiro_rt_W, 2)`, \textit{p} < 0.001), and the Levene's test showed that the variances in response times are not equal (\textit{F}(`r levenrt_df`, `r levenrt_dff`) = `r round(levenrt_F,2)`, \textit{p} = `r round(levenrt_p,2)`). To circumvent the possible issue with these tests [@Rochon2012; @shatz2023assumption], we employed visual inspections (i.e., Q-Q Plots). The visualizations also showed that the assumptions were violated. 

\begin{figure}
\centering
\makebox[\textwidth][c]{\includegraphics[width=0.8\textwidth]{comparison1.png}}
\captionsetup{labelformat=empty, font=scriptsize, justification=raggedright}
\caption[\textbf{\textit{Figure 3.}} Comparison between groups.]{\textbf{\textit{Figure 3.} Comparison between groups.} Plot A represents the average response time in minutes for each position by the participant. Most intermediates needed, on average, 2 minutes per position, whereas experts needed less than 1 minute. Plot B represents the total amount of correct answers for each individual.}
\end{figure}

$\hspace{1cm}$ For the differences in performance (see Figure 3), the WMW test showed that experts’ correct answers (\textit{M} = `r round(MExperts,1)`, \textit{SD} = `r round(SDExperts,1)`, \textit{Mdn} = `r MdnExperts`, \textit{IQR} = `r IQRExperts`) differed significantly from intermediates (\textit{M} = `r round(MBeginners,1)`, \textit{SD} = `r round(SDBeginners,1)`, \textit{Mdn} = `r MdnBeginners`, \textit{IQR} = `r round(IQRBeginners,2)`) (\textit{W} = `r round(m1_W,2)`, \textit{p} < 0.001, 95% CI [`r round(m1_CI[1], 2)`, `r round(m1_CI[2], 2)`]). The effect estimate suggests that an observation from the experts' group has approximately a 94% chance of having a higher score than from the intermediates group.

$\hspace{1cm}$ The response time for experts (\textit{M} = `r round(MExpertsRT,1)`, \textit{SD} = `r round(SDExpertsRT,1)`, \textit{Mdn} = `r round(MdnExpertsRT,2)`, \textit{IQR} = `r round(IQRExpertsRT,2)`) was also significantly different from the intermediates (\textit{M} = `r round(MBeginnersRT,1)`, \textit{SD} = `r round(SDBeginnersRT,1)`, \textit{Mdn} = `r round(MdnBeginnersRT,2)`, \textit{IQR} = `r round(IQRBeginnersRT,2)`) (\textit{W} = `r round(m2_W,2)`, \textit{p} = `r round(m2_p, 3)`, 95% CI [`r round(m2_CI[1], 2)`, `r round(m2_CI[2], 2)`]). The effect estimate suggests that an observation from the experts' group has approximately a 9% chance of having a higher response time than from the intermediates group. \textit{Post-hoc} power analyses are not meaningful here as they happen to be mainly driven by the \textit{p}-value (Lakens, 2022). The results\footnote{Recent literature also suggested the Brunner-Munzel’s test as an alternative to the WMW test (Karch, 2021). If interested, the results from this analysis were similar to the reported findings here and can be found in the GitHub repository.} supported our hypothesis 1 and hypothesis 2.

```{r bayes rank test, echo = FALSE, include = FALSE}
#from github
#if (!require("devtools")) install.packages("devtools")
#devtools::install_github("joereinhardt/BayesianFirstAid-Wilcoxon")
#this package seems to be quite outdated and from the message is still not completely finished. You do get results but unclear how trustworthy

#given that, i followed the implementation in JASP:
#1)for total score I assumed group 1 > group 2 (meaning that the experts would have higher score than intermediates)
#with BF10 and tests with mann-whitney (no. samples: 10000), and repeatability with set seed 1
#2)for response time I assumed groups 1 < group 2 (meaning that the experts would have higher score than intermediates)
#with BF10 and tests with mann-whitney (no. samples: 10000), and repeatability with set seed 1
```

$\hspace{1cm}$ The Bayesian alternative also supported our initial findings. For the correct answers, the posterior median for $\delta$ was 1.14, 95% CI [0.23, 2.21]. This median value represents the higher amount of correct answers for experts. The corresponding Bayes factor indicated the data was 14.96 more likely under the H1 than the H0 ($BF_{10}$ = 14.96), which shows substantial evidence against the hypothesis that experts and intermediates performed similarly. For the response times, the posterior median for $\delta$ was -1.04, 95% CI [-2.11, -0.17], representing that experts had lower response times. The corresponding Bayes factor indicated the data was 9.82 more likely under the H1 than H0 ($BF_{10}$ = 9.82), which yields substantial evidence against the hypothesis that experts and intermediates answered within similar time ranges.  

```{r, echo=FALSE, fig.cap="", message=FALSE, warning=FALSE, include=FALSE}
#necessary packages
library(ggplot2)
library(scales)

#THE NEXT STEPS ARE FOR THE DISTANCE MATRIX AND THEN MDS
#filtering data to have only one row per participant
unique_data <- sumstatsbyparticipant %>% distinct(participant, .keep_all = TRUE)
#selecting the relevant columns
mds_data <- unique_data[, c("key_resp_rt_minutes", "amount_correct")]
#standardizing the variables
mds_data_standardized <- scale(mds_data)
#mds_data_standardized
#distance matrix
distance_matrix <- dist(mds_data_standardized)
distance_matrix

#performing the MDS
mds_result <- cmdscale(distance_matrix)

#data frame with the MDS results and corresponding colors
mds_result_df <- data.frame(Dimension1 = mds_result[, 1], Dimension2 = mds_result[, 2], colors = ifelse(unique_data$rankings == 1, "firebrick", "dodgerblue2"), participant = unique_data$participant)

#MDS plot following APA norms
mds_plot <- ggplot(mds_result_df, aes(x = Dimension1, y = Dimension2, color = colors, label = participant)) +
  geom_point() +
  scale_color_manual(values = c("dodgerblue", "firebrick"), labels = c("Intermediates", "Experts")) +
  theme_minimal(base_size = 14) +
  theme(axis.title.x = element_text(size = 10), axis.title.y = element_text(size = 10), legend.position = c(.9,.9),
        axis.line.x = element_line(color = "black", size = .4),
        axis.line.y = element_line(color = "black", size = .4),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.box.background = element_rect(color = "black", size = .2)) +
  labs(x = "", y = "", color = "Expertise") +
  guides(color = guide_legend(title = NULL, override.aes = list(shape = c(16, 16)))) 

mds_plot

#NOT USED FOR THE THESIS MANUSCRIPT 
#checking the "fit"
#library(MASS)
#performing the MDS with isoMDS
#mds_result <- isoMDS(distance_matrix)
#printing the stress
#print(mds_result$stress)
```

```{r heatmap, echo = FALSE, include=FALSE}
#converting the distance matrix to a square matrix (RDM)
rdm <- as.matrix(distance_matrix)
#row and column names to the RDM
rownames(rdm) <- unique_data$rankings
colnames(rdm) <- unique_data$rankings

#print(rdm)
#heatmap(rdm)

library(gplots)
#color palette
my_palette <- colorRampPalette(c("dodgerblue", "white", "firebrick"))(n = 299)

#heatmap.2(rdm,
          #col = my_palette, 
          #trace = 'none', 
          #margin = c(12,12), 
          #srtCol = 0,  # Change this to 0 for straight labels
          #key.xlab = 'Dissimilarity', 
          #key.ylab = 'Frequency', 
          #main = '',
          #density.info = 'none',
          #symm = TRUE, 
          #dendrogram = 'row',
          #Rowv = TRUE,
          #Colv = TRUE)
```

```{r within and between consistency, echo = FALSE, include=FALSE}
#convert rankings to numeric for indexing
unique_data$rankings <- as.numeric(as.character(unique_data$rankings))

#generate indices for the groups
group0_indices <- which(unique_data$rankings == 0)
group1_indices <- which(unique_data$rankings == 1)

#relevant sections of the distance matrix
rdm_group0 <- rdm[group0_indices, group0_indices]
rdm_group1 <- rdm[group1_indices, group1_indices]
rdm_between_groups <- rdm[group0_indices, group1_indices]

#only the upper diagonal, excluding the main diagonal
rdm_group0_upper <- rdm_group0[upper.tri(rdm_group0, diag = FALSE)]
rdm_group1_upper <- rdm_group1[upper.tri(rdm_group1, diag = FALSE)]

#means and standard deviations
group0_mean <- mean(rdm_group0_upper, na.rm = TRUE)
group0_sd <- sd(rdm_group0_upper, na.rm = TRUE)

group1_mean <- mean(rdm_group1_upper, na.rm = TRUE)
group1_sd <- sd(rdm_group1_upper, na.rm = TRUE)

between_groups_mean <- mean(rdm_between_groups, na.rm = TRUE) 
between_groups_sd <- sd(rdm_between_groups, na.rm = TRUE)  

#storing results
withinnovices <- group0_mean
withinnovicessd <- group0_sd

withinexperts <- group1_mean
withinexpertssd <- group1_sd

betweengroups <- between_groups_mean
betweengroupssd <- between_groups_sd

# If you want the lower diagonal, use this instead
# rdm_group0_lower <- rdm_group0[lower.tri(rdm_group0, diag = FALSE)]
# rdm_group1_lower <- rdm_group1[lower.tri(rdm_group1, diag = FALSE)]

# Calculate means and standard deviations
# group0_mean

#x and y limits for the plot
xlim_lower <- 1
xlim_upper <- max(length(rdm_group0_upper), length(rdm_group1_upper)) #max length of both vectors
ylim_lower <- min(c(rdm_group0_upper, rdm_group1_upper)) #min value across both vectors
ylim_upper <- max(c(rdm_group0_upper, rdm_group1_upper)) #max value across both vectors

# Creating the plot with explicit x and y limits
plot(1:length(rdm_group0_upper), rdm_group0_upper, col = "blue", type = "b",
     xlab = "Pairwise combinations", ylab = "Distance", main = "", 
     xlim = c(xlim_lower, xlim_upper), ylim = c(ylim_lower, ylim_upper)) 

#points to the plot
points(1:length(rdm_group1_upper), rdm_group1_upper, col = "red", type = "b")

#adding a legend to the plot
legend("topleft", legend=c("Intermediates", "Experts"),
       col=c("blue", "red"), lty = 1, cex=0.8)

#the upper results into vectors from both groups. there was probably a more efficient way of selecting them 
rdm_group1_upper <- c(1.157190702, 1.165812716, 0.058103492, 0.640787847, 0.591261210, 0.580130779, 1.148758019, 0.110277427, 0.168380919, 0.626820604, 1.150855255, 0.066404295, 0.124507787, 0.610539451, 0.043873132, 1.156355038, 0.006912311, 0.065015803, 0.592950514, 0.103365115, 0.059491983, 1.247586639, 0.631072831, 0.689176323, 0.962351017, 0.520795405, 0.564668537, 0.624160520, 1.149174472, 0.098052150, 0.156155642, 0.622015730, 0.012225276, 0.031647856, 0.091139839, 0.533020681, 1.153323281, 0.035599204, 0.093702696, 0.600761035, 0.074678222, 0.030805090, 0.028686893, 0.595473627, 0.062452946)

rdm_group0_upper <- c(0.2873820, 2.4430228, 2.7187834, 1.9037979, 2.0592860, 2.7699109, 1.4363863, 1.7237594, 1.1610620, 1.7260978, 1.8281931, 2.0969994, 1.5557839, 1.2343399, 0.7263973, 2.5131438, 2.7568024, 2.1342677, 1.1922505, 1.5295552, 0.8078051, 2.9419569, 3.0536114, 3.7232542, 1.1009306, 2.7779303, 2.1730123, 1.7332569, 0.2478214, 0.2370481, 2.6506007, 1.8223406, 1.5995842, 1.9106435, 2.5439385, 2.8198875)

#BELOW NOT USED FOR THE THESIS MANUSCRIPT
#observed difference
obs_diff <- mean(rdm_group0_upper) - mean(rdm_group1_upper) 
obs_diff

library(boot)
compute_mean <- function(data, indices) {
  mean(data[indices], na.rm = TRUE)
}

#bootstrap the mean for group 0
boot_group0 <- boot(data = rdm_group0_upper, statistic = compute_mean, R = 10000)

#bootstrap the mean for group 1
boot_group1 <- boot(data = rdm_group1_upper, statistic = compute_mean, R = 10000)

#bootstrap the mean for between groups
boot_between_groups <- boot(data = rdm_between_groups, statistic = compute_mean, R = 10000)
boot_between_groups

group0_boot_mean <- mean(boot_group0$t, na.rm = TRUE)
group0_boot_sd <- sd(boot_group0$t, na.rm = TRUE)

group1_boot_mean <- mean(boot_group1$t, na.rm = TRUE)
group1_boot_sd <- sd(boot_group1$t, na.rm = TRUE)

between_groups_boot_mean <- mean(boot_between_groups$t, na.rm = TRUE)
between_groups_boot_sd <- sd(boot_between_groups$t, na.rm = TRUE)

#intervals for group 0
ci_group0 <- boot.ci(boot_group0, type = "bca")

#intervals for group 1
ci_group1 <- boot.ci(boot_group1, type = "bca")

#intervals for between groups
ci_between_groups <- boot.ci(boot_between_groups, type = "bca")

lower_limit_group0 <- ci_group0$bca[4]
upper_limit_group0 <- ci_group0$bca[5]

lower_limit_group1 <- ci_group1$bca[4]
upper_limit_group1 <- ci_group1$bca[5]

lower_limit_between_groups <- ci_between_groups$bca[4]
upper_limit_between_groups <- ci_between_groups$bca[5]
```

\begin{figure}
\centering
\makebox[\textwidth][c]{\includegraphics[width=0.8\textwidth, height=0.4\textheight]{heatmap.png}}
\captionsetup{labelformat=empty, font=scriptsize, justification=raggedright}
\caption[\textbf{\textit{Figure 4.}} Dissimilarity matrix]{\textbf{\textit{Figure 4.} Dissimilarity matrix.} The 1’s represent experts, whereas the 0’s represent intermediates. The values closer to 3 are more dissimilar, whereas the values closer to 0 are less dissimilar. The participants were hierarchically clustered, meaning that the values were grouped together based on their similarity.}
\end{figure}

$\hspace{1cm}$ The matrix rendered as a heatmap shows the dissimilarity patterns between both groups (see Figure 4). Based on the within-group dissimilarities, the experts showed more consistency as supported by a lower average dissimilarity (\textit{M} = `r round(withinexperts,2)`, \textit{SD} = `r round(withinexpertssd,2)`) than intermediates (\textit{M} = `r round(withinnovices,2)`, \textit{SD} = `r round(withinnovicessd,2)`). The between-group dissimilarity further showed the difference between both groups (\textit{M} = `r round(betweengroups,2)`, \textit{SD} = `r round(betweengroupssd,2)`), indicated by the higher average dissimilarity. Therefore, in line with previous studies (Duyck et al., 2021), the results showed that experts solved the task more consistently, which supports our hypothesis 3.

$\hspace{1cm}$ The findings from the MDS analysis also revealed a higher consistency among experts (see Figure 5). More specifically, the MDS showed a configuration of points that highlighted a cluster of experts in contrast to intermediates, which suggests a more homogeneous performance among experts. From this same analysis, we could confirm that the two dimensions underlie the perceived similarities between our participants. Experts are more similar based on the response time and number of questions answered correctly, even if the dimensions are not directly corresponding to our original variables. 

\begin{figure}
\centering
\makebox[\textwidth][c]{\includegraphics[width=0.8\textwidth, height=0.4\textheight]{multidimensionalscaling.png}}
\captionsetup{labelformat=empty, font=scriptsize, justification=raggedright}
\caption[\textbf{\textit{Figure 5.}} Multidimensional scaling.]{\textbf{\textit{Figure 5.} Multidimensional scaling.} The plot represents each participant's performance in a two-dimensional space. The relative distances between points demonstrate the similarity in their performance, with closer points suggesting more similar response times and accuracy. The horizontal and vertical axes are respectively the first and second dimension computed by the MDS method.}
\end{figure}

```{r percentage, echo = FALSE, include = FALSE, eval=FALSE}
#vector with easy question numbers
easy_questions <- c(3, 6, 12, 15, 22, 24)

#new column based on whether "questions" is in the easy questions list
data$Question_Difficulty <- ifelse(data$questions %in% easy_questions, "Easy", "Hard")

#factorizing the column 
data$Question_Difficulty <- as.factor(data$Question_Difficulty)
#hard' the reference level
data$Question_Difficulty <- relevel(data$Question_Difficulty, ref = "Hard")

percentage_summary <- data %>%
  group_by(rankings) %>%
  summarise(
    easy_correct_percentage = sum(Question_Difficulty == "Easy" & Answer == 1) / 
      sum(Question_Difficulty == "Easy") * 100,
    hard_correct_percentage = sum(Question_Difficulty == "Hard" & Answer == 1) / 
      sum(Question_Difficulty == "Hard") * 100,
    easy_incorrect_percentage = sum(Question_Difficulty == "Easy" & Answer == 0) / 
      sum(Question_Difficulty == "Easy") * 100,
    hard_incorrect_percentage = sum(Question_Difficulty == "Hard" & Answer == 0) / 
      sum(Question_Difficulty == "Hard") * 100
  )

percentage_summary
```

\begin{figure}
\centering
\makebox[\textwidth][c]{\includegraphics[width=0.8\textwidth, height=0.4\textheight]{percentagewithseforposition.png}}
\captionsetup{labelformat=empty, font=scriptsize, justification=raggedright}
\caption[\textbf{\textit{Figure 6.}} Percentage of correct responses between intermediates and experts across questions.]{\textbf{\textit{Figure 6.} Percentage of correct responses between intermediates and experts across questions.} The error bar represent the standard error of the mean.}
\end{figure}

```{r percentage for all questions, echo = FALSE, include = FALSE, eval=FALSE}
differences <- inner_join(x = nonrankedplayers, y = rankedplayers, by = "questions") 

differences <- differences %>%
  mutate(difference = scoreR - scoreNR) %>%
  select(questions,scoreR,scoreNR, difference)

ggplot(differences, aes(x= questions , y=difference, fill = difference)) + 
  geom_bar(stat = "identity", alpha=1)+
  scale_x_discrete("Question", breaks = seq(0,26, by = 1))+
  scale_y_continuous("Difference in score: ranked vs non-ranked", breaks = seq(0,1, by = 0.1)) +
  ggtitle("Difference in performance between groups per question")+
  theme(plot.title = element_text(hjust = 0.5)) #centre the title
  

differences <- differences %>% 
  mutate(level = case_when(questions %in% c(3,6,12,15,22,24) ~ "easy", 
                               !questions %in% c(3,6,12,15,22,24) ~ "hard"))

average_differences <- differences %>% 
  group_by(level) %>% 
  summarise(average_difference = mean(difference, na.rm = TRUE))

#NOT USED FOR THE MANUSCRIPT
#ggplot(differences, aes(x= questions , y=difference, fill = level)) + 
  #geom_bar(stat = "identity", alpha=1)+
  #scale_x_discrete("Question", breaks = seq(0,26, by = 1))+
  #scale_y_continuous("Difference in score: ranked vs non-ranked", breaks = seq(0,1, by = 0.1)) +
  #ggtitle("Difference in performance between groups per question")+
  #theme(plot.title = element_text(hjust = 0.5)) #centre the title
```

$\hspace{1cm}$ Furthermore, based on the same behavioral judgments, we were able to identify the positions that better distinguished experts and intermediates (see Figure 6). We found notable differences in the average proportion of correct answers between groups for the displayed questions. Indeed, we could observe that the difference was approximately 0.45 for hard questions, which indicates that experts answered correctly, on average, 45% more than intermediates. For the easy questions this difference was only 9%, meaning that experts answered correctly, on average, only 9% more than intermediates (see Supplementary Materials for a detailed table). This finding was in line with our hypothesis 4. 

$\hspace{1cm}$ Contrary to our expectations, there was a similar performance for question 20, meaning that this position underperformed in comparison to the others. In this case, we decided to not include this position in the stimuli set for the future fMRI study.

```{r plot wilcoxon, echo = FALSE, fig.cap="Comparison between groups", warning=FALSE, include=FALSE}
#calculate the adjusted key response time
data <- data %>%
  mutate(key_resp_rt_minutes = key_resp.rt / 60)

#plot key_resp_rt_adjusted
key_resp_plot <- ggplot(sumstatsbyparticipant, aes(x = factor(rankings), y = key_resp_rt_minutes, fill = factor(rankings))) +
  geom_violin(trim = FALSE, alpha = 0.4) +  #violin plot
  geom_boxplot(alpha = 0.6, width = 0.2, position = position_nudge(x = 0.2)) +  #boxplot and nudge it to the right
  scale_fill_manual(values = c("dodgerblue2", "firebrick"), guide = FALSE) +  #remove legend
  labs(
    title = "",
    x = "Expertise group",
    y = "Response Time (m)"
  ) +
  scale_x_discrete(labels = c("Intermediates", "Experts")) +  # Update x-axis labels
  theme_minimal() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = "black")) +
  scale_y_continuous(limits = c(0, max(10)))

#plot total_correct
total_correct_plot <- ggplot(data, aes(x = factor(rankings), y = total_correct, fill = factor(rankings))) +
  geom_violin(trim = FALSE, alpha = 0.4) +  #violin plot
  geom_boxplot(alpha = 0.6, width = 0.2, position = position_nudge(x = 0.2)) +  #boxplot and nudge it to the right
  scale_fill_manual(values = c("dodgerblue2", "firebrick"), guide = FALSE) +  #remove legend
  labs(
    title = "",
    x = "Expertise group",
    y = "Total Correct"
  ) +
  scale_x_discrete(labels = c("Intermediates", "Experts")) +  #updating x-axis labels
  theme_minimal() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = "black")) +
  scale_y_continuous(limits = c(0, max(26)))

#combine the plots side by side
combined_plot <- plot_grid(key_resp_plot, total_correct_plot, labels = "AUTO", ncol = 2) 
#print the combined plot
#combined_plot
```

```{r plot bars, echo = FALSE, include = FALSE}
#joining the two data frames
combined_data <- nonrankedplayers %>%
  left_join(rankedplayers, by = "questions", suffix = c("_NR", "_R"))

#new data frame with only the necessary columns
#plot_data <- combined_data %>% select(questions, scoreNR, scoreR)
# Print the data as a table
#print(plot_data)

#dumbbell plot
ggplot() +
  geom_point(data = combined_data, aes(x = questions, y = scoreNR), color = "dodgerblue2", size = 3) +
  geom_point(data = combined_data, aes(x = questions, y = scoreR), color = "firebrick", size = 3) +
  geom_segment(data = combined_data,
               aes(x = questions, xend = questions, y = scoreNR, yend = scoreR),
               color = "black", size = 0.5) +
  theme_classic() +
  ggtitle("") +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_x_discrete("Question", breaks = seq(0, 26, by = 1)) +
  scale_y_continuous("Percentage of correct answers", breaks = seq(0, 1, by = 0.1))
```

```{r plot with se, eval=FALSE, echo = FALSE, include = FALSE}
# Add group to the data frames
rankedplayers$group <- "Experts"
nonrankedplayers$group <- "Intermediates"

#cmbine the two data frames
#combined_data <- rbind(nonrankedplayers, rankedplayers) 
#might have issues because named the columns with NR or R
#I solved this in a separate R file

#plot
ggplot(combined_data, aes(x= questions, y=score, fill=group)) + 
  geom_bar(stat="identity", position="dodge", alpha=1) +
  scale_x_discrete("Question", breaks = seq(0,26, by = 1), expand = expansion(add = c(0.6, 0))) +  # add a bit of space before the first bar
  scale_y_continuous("Percentage of correct answers", breaks = seq(0,1, by = 0.1), expand = expansion(mult = c(0, 0.05))) +
  scale_fill_manual(values=c(Intermediates="lightblue", Experts="lightpink2")) + # change colors
  labs(fill = "") + # remove the title of the legend
  ggtitle("") +
  theme(plot.title = element_text(hjust = 0.5), # centre the title 
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank(),
        axis.line = element_line(colour = "black")) +
  geom_errorbar(aes(x=questions, ymin=min, ymax=max), width=0.4, position=position_dodge(0.9), colour="black", alpha=0.9, size=0.5) +
  geom_hline(yintercept = 0, color = "black")  # adding a line at y=0
```

```{r logistic regression, echo = FALSE, include=FALSE}
#vector with easy question numbers
easy_questions <- c(3, 6, 12, 15, 22, 24)

#new column based on whether "questions" is in the easy questions list
data$Question_Difficulty <- ifelse(data$questions %in% easy_questions, "Easy", "Hard")

#factorizing the column 
data$Question_Difficulty <- as.factor(data$Question_Difficulty)
#hard' the reference level
data$Question_Difficulty <- relevel(data$Question_Difficulty, ref = "Hard")

selected_data <- data %>%
  select(Answer, Question_Difficulty, rankings, participant)

#STEP FROM https://ladal.edu.au/regression.html#Mixed-Effects_Binomial_Logistic_Regression
#generate fixed-effects minimal base-line models and a base-line mixed-model
baseline_model <- glm(Answer ~ 1, 
               family = binomial, 
               data = data)
baseline_mixed <- glmer(Answer ~ (1 | participant), 
               family = binomial, 
               data = data)
modelrankings <- glmer(Answer ~ rankings + (1 | participant), 
               family = binomial, 
               data = data)
modelquestions <- glmer(Answer ~ Question_Difficulty + (1 | participant), 
               family = binomial, 
               data = data)
modelinteraction <- glmer(Answer ~ Question_Difficulty * rankings + (1 | participant), 
               family = binomial, 
               data = data)
#anova(modelrankings, model, modelinteraction)

#If the AIC of the glmer object is smaller than the AIC of the glm object, 
#then this indicates that including random intercepts is justified.

#MAIN MODEL DEFINED HERE (MIXED-EFFECTS)
model <- glmer(Answer ~ Question_Difficulty + rankings + (1 | participant), 
               family = binomial, 
               data = data)
#summary information
model_summary <- summary(model)

############
#ASSUMPTIONS
library(lattice) #for dotplot
library(DHARMa)
testData = createData(sampleSize = 494, overdispersion = 0, fixedEffects = 2, family = binomial(), randomEffectVariance = 19, numGroups = 2)
simulationOutput <- simulateResiduals(fittedModel = model)
plot(simulationOutput)
plotResiduals(simulationOutput)
testDispersion(simulationOutput)
testZeroInflation(simulationOutput)
plotResiduals(simulationOutput, testData$Environment1, quantreg = T)

qqnorm(resid(model))
qqline(resid(model))

dotplot(ranef(model))
############
############
#everything seems in order according to the simulation-based visualizations

#THIS WAS USED FOR THE MANUSCRIPT FOR PART R^2 BUT I HAVE WITH # TO PREVENT FROM RUNNING EVERYTIME I KNIT
#calculate part R-squared with 1000 bootstrap iterations
#part_r2 <- partR2(modelx, partvars = c("Question_Difficulty", "rankings"), data = data, nboot = 1000)
#print(part_r2)

#original output report:
#R2 (marginal) and 95% CI for the full model: 
 #R2     CI_lower CI_upper nboot ndf
 #0.4137 0.2229   0.7814   1000  3  

#Part (semi-partial) R2:
 #Predictor(s)                 R2     CI_lower CI_upper nboot ndf
 #Model                        0.4137 0.2229   0.7814   1000  3  
 #Question_Difficulty          0.1484 0.0000   0.6778   1000  2  
 #rankings                     0.3324 0.1350   0.7497   1000  2  
 #Question_Difficulty+rankings 0.4137 0.2229   0.7814   1000  1  
################

#RELEVANT VALUES FOR REPORTING AND TABLES
#FIXED EFFECTS
#extracting the relevant values based on the output 
#fixed effects
fixed_effects <- coef(model)
#random effects
random_effects <- ranef(model)
model_summary$coefficients
#fixed effects summary
fixed_effects_summary <- summary(model)$coefficients

#extract estimates, standard errors, and p-values for rankings1
rankings_estimate <- fixed_effects_summary["rankings1", "Estimate"]
rankings_se <- fixed_effects_summary["rankings1", "Std. Error"]
rankings_pvalue <- fixed_effects_summary["rankings1", "Pr(>|z|)"]

#extract estimates, standard errors, and p-values for Question_DifficultyEasy
questions_estimate <- fixed_effects_summary["Question_DifficultyEasy", "Estimate"]
questions_se <- fixed_effects_summary["Question_DifficultyEasy", "Std. Error"]
questions_pvalue <- fixed_effects_summary["Question_DifficultyEasy", "Pr(>|z|)"]

#in odds ratio
rankings_odds <- exp(rankings_estimate)
questions_odds <- exp(questions_estimate)

#RANDOM EFFECTS
#extract variance of random effects
random_effects_variance <- VarCorr(model)
#print variance of random effects
print(random_effects_variance)
#extract individual random effects
random_effects_individual <- ranef(model)
#print individual random effects
print(random_effects_individual)

#variance for participant intercept
participant_variance <- as.data.frame(random_effects_variance)$vcov[1]
participant_variance
#standard deviation for participant intercept
participant_std_dev <- as.data.frame(random_effects_variance)$sdcor[1]
participant_std_dev

#To compare the two models, we use a likelihood ratio test: this compares the likelihood of one model to the likelihood of another 
#(i.e., how plausible the parameter values are, given the data). The two models are nested
modelwithoutquestion <- glmer(Answer ~ rankings + (1 | participant), 
               family = binomial, 
               data = data)
likelihoodrt <- anova(model, modelwithoutquestion)
chisquarelr <- likelihoodrt$Chis[2]
deviancelr <- likelihoodrt$deviance[2]
dfchisquare <- likelihoodrt$Df[2]

###############logistic regression
modellogisticonly <- glm(Answer ~ Question_Difficulty + rankings,
             family = binomial,
             data = data)
#summary(modellogisticonly)
```

```{r auc, echo = FALSE, include=FALSE}
library(pROC)
#EVALUATIONG THE MODEL WITH ROC
#predicted probabilities
data$predicted_prob <- predict(model, data, type = "response")
#ROC curve
roc_obj <- roc(data$Answer, data$predicted_prob)
#plotting the curve
plot(roc_obj, main="")
#line
abline(h = 0.5, lty = 2, col = "gray")
#the AUC (Area Under the Curve)
#auc <- auc(roc_obj)
auc <- 0.92
#confidence interval 
ci_auc <- ci.auc(roc_obj)
ci_aucfirst <- ci_auc[1]
ci_aucsecond <- ci_auc[3]

#EVALUATING THE MODEL WITH CONFUSION MATRIX
#a better way of evaluate the model
data$predicted_prob <- predict(model, data, type = "response")
library(caret)
predicted_class <- ifelse(data$predicted_prob > 0.5, 1, 0)
confusionMatrix(as.factor(predicted_class), as.factor(data$Answer))
```

$\hspace{1cm}$ The mixed-effects logistic regression model further developed on the descriptive findings. To assess the overall predictive performance of our model, we first calculated the area under the ROC curve (AUC = `r round(auc,2)`, 95% CI [`r round(ci_aucfirst,2)`, `r round(ci_aucsecond,2)`]), which yielded a high level of discrimination. We also created a confusion matrix to provide more information about the prediction accuracy. According to it, the model sustains an overall accuracy of 0.87, 95% CI [0.83, 0.90]. However, our primary interest lied in understanding the effects of the questions’ difficulty level on the probability of a correct answer. The confusion matrix and ROC curve were used solely for assessing the predictive performance of our model, not to directly address our research question. For this, the estimated coefficients and predicted probabilities were used.

```{r plot predictions, eval=FALSE, echo = FALSE, include = FALSE}
library(ggplot2)
# Get the estimated marginal means
emmeans_results <- emmeans(model, ~ Question_Difficulty + rankings)

# Print the results
print(emmeans_results)

# Get the estimated marginal means on the probability scale
emmeans_results_prob <- summary(emmeans_results, type = "response")

# Print the results
print(emmeans_results_prob)

emmeans_df <- as.data.frame(emmeans_results_prob)

predictedplot <- ggplot(emmeans_df, aes(x = Question_Difficulty, y = prob, color = as.factor(rankings))) +
  geom_point(size = 1.5) +
  scale_color_manual(values = c("0" = "dodgerblue2", "1" = "firebrick"),
                     name = "Group",
                     labels = c("Intermediates", "Experts")) +
  geom_errorbar(aes(ymin = asymp.LCL, ymax = asymp.UCL), width = 0.2, colour = "lightgrey", alpha = 0.5) +
  labs(x = "Question Difficulty", 
       y = "Predicted Probability of a Correct Answer",
       color = "Rankings",
       title = "") +
  theme_bw() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        plot.title = element_text(size = 12, face = "bold", hjust = 0.5),
        axis.title.x = element_text(size = 10, face = "bold"),
        axis.title.y = element_text(size = 10, face = "bold"),
        axis.text.x = element_text(size = 10),
        axis.text.y = element_text(size = 10),
        legend.text = element_text(size = 10),
        legend.title = element_text(size = 10, face = "bold"),
        axis.line.x = element_line(color = "black", size = 0.5),
        axis.line.y = element_line(color = "black", size = 0.5)) 
#predictedplot
```

```{r values for predicted prob, echo = FALSE, include=FALSE}
predictedexpertseasy = 0.99
predictedexpertshard = 0.98

predictedintermediateshard = 0.49
predictedintermediateseasy = 0.93
```

$\hspace{1cm}$ The random effects indicated that there is considerable variability across different participants (variance = 3.47, std. deviation = 1.86). The fixed effects estimates showed that the experts group had a significant positive effect on the likelihood of a correct answer (\textit{B} = `r round(rankings_estimate,2)`, \textit{SE} = `r round(rankings_se,2)`, \textit{p} < 0.001, semi-partial $R^2$ = 0.33, 95% CI [0.13, 0.74]). The odds of a correct answer are approximately `r round(rankings_odds,2)` ($e^{4.59}$) times higher for experts compared to intermediates (reference level), assuming all other variables remain constant. This group difference in the behavioral task was expected and, therefore, presents itself as circular to certain extent: experts by definition should outperform intermediates. Yet, it is important as a verification that our stimulus set is able to capture the difference in expertise. Moreover, the easy questions had a significant positive effect on the likelihood of a correct answer (\textit{B} = 2.69, \textit{SE} = `r round(questions_se,2)`, \textit{p} < 0.001, semi-partial $R^2$ = 0.14, 95% CI [0.00, 0.67]\footnote{The part (semi-partial) $R^2$ computation followed the description in Stoffel et al. 2021 for the R package \textit{partR2} (Version: 0.9.1), which involved parametric bootstrapping to quantify the sampling variance.}. The odds of a correct answer are `r round(questions_odds,2)` ($e^{2.69}$) times higher for the easy questions compared to the hard question (reference level), assuming all other variables remain constant. The LRT revealed a significant improvement in model fit ($x$²(`r dfchisquare`) = `r round(chisquarelr,2)`, \textit{p} < 0.001).

$\hspace{1cm}$ The predicted probabilities offered a more detailed finding related to the difficulty level. For experts, the probability of a correct response is approximately `r round(predictedexpertshard,2)` for hard questions and `r round(predictedexpertseasy,2)` for easy questions. For intermediates, the probability of a correct response for hard questions is `r round(predictedintermediateshard,2)`, whereas the probability for easy questions is `r round(predictedintermediateseasy,2)` (see Figure 7). Therefore, our findings\footnote{We compared the model described here with another model that included the interaction term between both predictors. The latter had the following indices: AIC (321.71), BIC (342.72), and log-likelihood (-155.85). This model did not fit the data better ($x^2$(1) = 1.12, p = 0.289).} also supported our hypothesis 4.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(kableExtra)
library(dplyr)

#model defined again
model <- glmer(Answer ~ Question_Difficulty + rankings + (1 | participant), 
               family = binomial, 
               data = data)

#fixed effects data frame
fixed_effects_summary <- summary(model)$coefficients
fixed_effects_df <- data.frame(
  Term = rownames(fixed_effects_summary),
  Estimate = round(fixed_effects_summary[, "Estimate"], 3),
  SE = round(fixed_effects_summary[, "Std. Error"], 3),
  z_value = round(fixed_effects_summary[, "z value"], 3),
  p_value = fixed_effects_summary[, "Pr(>|z|)"],
  stringsAsFactors = FALSE
)
fixed_effects_df$p_value = ifelse(fixed_effects_df$p_value < 0.001, "< 0.001", round(fixed_effects_df$p_value, 3))

# Change the fixed effect names
names <- c("Intercept", "Easy Questions", "Experts")
fixed_effects_df$Term <- names

# Generate fixed effects table
fixed_effects_table <- kable(fixed_effects_df, "latex", align = "lcccc", 
      row.names = FALSE, booktabs = TRUE, 
      col.names = c("Term", "Estimate", "SE", "z value", "p-value"),
      caption = "Fixed effects") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

#print
fixed_effects_table 
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(kableExtra)
library(dplyr)
#model fit information dataframe
model_info <- data.frame(
  AIC = AIC(model),
  BIC = BIC(model),
  logLik = as.numeric(logLik(model)),
  Deviance = deviance(model)
)

# Generate model fit information table
model_info_table <- kable(model_info, "latex", align = "cccc", 
                          row.names = FALSE, booktabs = TRUE, 
                          col.names = c("AIC", "BIC", "logLik", "Deviance"),
                          caption = "Model fit indices") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

#print
model_info_table
```

\begin{figure}
\centering
\makebox[\textwidth][c]{\includegraphics[width=0.8\textwidth]{logisticregressionquestionstogether2.png}}
\captionsetup{labelformat=empty, font=scriptsize, justification=raggedright}
\caption[\textbf{\textit{Figure 7.}} Predicted probabilities.]{\textbf{\textit{Figure 7.} Predicted probabilities.} The predicted probability is calculated by converting the estimates on the logit scale to probabilities.}
\end{figure}

\subsection*{Discussion}
\addcontentsline{toc}{subsection}{Discussion}

$\hspace{1cm}$ The current behavioral study adds to the literature as further evidence for a clear distinction between experts and intermediates. More importantly, it allowed us to move forward into designing the fMRI experiment. Our findings showed that experts answered more questions correctly and demanded less time for it. We also showed that experts demonstrated more consistency throughout the behavioral task. Lastly, we designed and successfully tested that our stimuli capture this difference between both groups. Previous research investigating the behavioral performance of experts (i.e., mineralogist and ornithologists) on a variety of tasks has shown similar evidence for domain-specific skills. The findings from Martens et al. (2018) support that expertise affects performance on discrimination and semantic tasks related to the specific domain of expertise. Duyck et al. (2021) showed that experts also exhibit a higher degree of consistency in a Multiple Object Arrangement (MOA) task in comparison to controls. Furthermore, the above-mentioned studies also suggest that experts’ knowledge may be structured in a way that facilitates the efficient categorization and identification of objects of expertise in the brain, following similar behavioral results as we obtained in this section.

\newpage 

\sectionfont{\centering}
\begingroup
\Large
\bfseries
\section*{Section II: fMRI study}
\addcontentsline{toc}{section}{Section II: fMRI study}
\endgroup

\subsection*{Main objectives}
\addcontentsline{toc}{subsection}{Main objectives}

$\hspace{1cm}$ This section explored the extent to which the manifold framework accounts for expertise-related changes. We designed the study to examine the degree and regions where visual expertise changes the way in which the expertise objects are represented in the brain. With MVPA, we will differentiate the distributed neural responses associated with chess positions in order to later combine the same representations in a representational space, which will serve to compare novices and experts at the neural and behavioral levels. 

$\hspace{1cm}$ In order to analyze the representations' differences at the subordinate level, we were interested in dissociating between the low-level and high-level features of our stimuli. This means that we included the non-checkmate versions of all positions in this study. We hypothesized that the neural response pattern will contain the information related to the low-level dimensional aspect (i.e., visual similarity) in both groups. But, in the case of novices, we presumed that they will demonstrate a more closely aligned neural response pattern to this low-level dimension compared to their expert counterparts. In comparison, experts will also display a neural response pattern related to the high-level dimensions (H1), besides the low-level information. We also hypothesized that the neural response pattern of expert players will be more distinctive and consistent relative to novices (H2), as Duyck et al. (2021) found in the context of ornithology (birds expertise). Lastly, we hypothesized that the more refined neural representations in experts will be found in the neural response pattern in the following brain regions: LOC, FFA, PPA, TPJ, dorsolateral prefrontal cortex (DLPFC), and PCC (H3). The ROIs for this study in contrast to previous research (Duyck et al., 2021) are theory-driven.

$\hspace{1cm}$ The FFA has been linked to holistic processing in chess experts (Bilalić et al., 2011) and increased activity in response to other expertise-related objects [@Mcgugin2012; @bartlett2013expertise], although there are studies that have failed to find this association in other expertise domains (Martens et al., 2018). Indeed, the debate related to the neural activation in FFA when participants view expertise-related objects is still ongoing. The LOC and PPA are important as they process objects and scenes, which may be relevant to process chessboard positions. The TPJ is crucial for creating a holistic perception [@Huberle2012], and has demonstrated higher activation in chess experts compared to novices (Rennig et al., 2013). The PCC has shown significant differences between expert and non-expert players when processing realistic versus non-realistic chess positions (Krawczyk et al., 2011), with experts displaying higher activation in this area when viewing realistic positions (Bartlett et al., 2013). Lastly, the DLPFC is relevant to working memory [@Petrides2000] and related to visual expertise [@Moore2006; @duyck2021visual].

$\hspace{1cm}$ The procedure described here is based on the four pilot sessions conducted. Throughout this section, we use mainly the future tense based on the implemented procedure and observed outcome from these same pilot sessions. That said, the results reported here should be cautiously interpreted and not seen as an idiographic approach. The pilot sessions served to evaluate if the implementation of localizers would eventually improve the quality of the data. Furthermore, the sessions also provided an initial indication about the experimental design suitability to investigate all of our hypotheses. 

\subsection*{Methods}
\addcontentsline{toc}{subsection}{Methods}

$\hspace{1cm}$ \textbf{\textit{Participants.}} Based on prior studies (Duyck et al., 2021; Martens et al., 2018), we will include between 20 to 25 participants per group. The recruitment of chess experts started during the 33th Open Leuven 2022, a FIDE-approved chess tournament. In this case, due to the limited availability of experts with a rating over 2000 Elo, we adjusted the minimum threshold to 1800 Elo. For the novices, our recruitment will solely target participants who play chess casually (Bilalić, 2010), in contrast to the sample used for the behavioral section. Before the submission of this thesis, only four pilot fMRI sessions were performed, which included one expert. The procedures described here were approved by the ethical committee of the Catholic University of Leuven and the pilot participants provided written informed consent.

$\hspace{1cm}$ \textbf{\textit{Stimulus set.}} For the experimental task, 40 stimuli will be presented (see Appendix). Here we will include the non-checkmate and the reversed versions of the previously tested stimuli. More specifically, the set will consist of the 20 chess positions with a checkmate and the 20 visually similar versions in which the position of a single chess piece is different. The reversed stimuli represent the same chess positions tested in section I, but with the pieces horizontally moved to the opposite side of the board.

$\hspace{1cm}$ In the localizer runs, the first localizer will consist of 18 stimuli per category (i.e., faces, objects, scenes, and scrambled). For the second localizer run, we will also use 18 stimuli per category, but this time the categories will be: legal positions, illegal\footnote{The terms legal and illegal denote chess positions that are plausible and unplausible, respectively.} (i.e., no kings), and illegal-random (i.e., positions with randomized pieces), and scrambled images. All stimuli will be sized at 498 x 498 pixels and displayed at a visual angle of 8°. A behavioral pilot (not reported here) was conducted to determine the parameters that would allow participants to observe the entire chessboard while maintaining their focus on the board's center, where the fixation cross is located.

$\hspace{1cm}$ \textbf{\textit{Scanning procedure.}} The data acquisition plan, based on the pilot sessions, will consist of six experimental runs, three localizer runs (localizer 1 twice and localizer 2 once), and one anatomical scan. The further elaborated parameters are also based on these pilot sessions. 

$\hspace{1cm}$ Prior to the scanning day, participants will be invited to do a similar behavioral experiment as described in section I. The study will be sent to them online. They will evaluate (find the checkmate if available) the same 40 stimuli that will be later shown in the experimental runs. During the scanning session, participants will perform a familiarization task with the same set of chess positions before entering the scanner.

$\hspace{1cm}$ Concerning the experimental runs, we will make use of a rapid event-related design [@Huettel2012event] with a fixed inter-stimulus-interval (ISI) and randomized stimulus presentation. Each experimental run will include a random sequence of the 40 chessboard positions and a fixation point. In the pilot sessions, each run initiated and ended with a grey full-screen with a fixation cross in the middle (presented for 10 seconds). This was followed by the random presentation of each image twice (2.5 seconds display per stimulus) followed by a inter-stimulus interval (1 second). Each pilot run lasted for 4.98 minutes in total. The pilot participants received the instructions to fixate accordingly to the fixation cross and, on each trial, to press a button in order to indicate whether they prefer playing with the current chessboard position or the antecedent position they had seen (i.e., one-back task). Future participants will receive the same instructions. We will also monitor participant's fixations with an eyelink1000 long range eyetracking system.

\begin{figure}
\centering
\makebox[\textwidth][c]{\includegraphics[width=0.8\textwidth]{scanningdesign.png}}
\captionsetup{labelformat=empty, font=scriptsize, justification=raggedright}
\caption[\textbf{\textit{Figure 8.}} Schematic overview of a run for the experimental task.]{\textbf{\textit{Figure 8.} Schematic overview of a run for the experimental task.} This represents a one-back task. Participants will indicate which position they would prefer to play as white: the current or the previous chessboard. Button presses will be counterbalanced across runs.}
\end{figure}

$\hspace{1cm}$ For the localizer runs, we will use a block design with different stimulus types. Both localizers will be initiated and end with a grey full-screen with a fixation cross in the center. The tasks will also be identical across the localizers. The participants will be instructed to fixate their gaze on the cross and press a button whenever an image is presented twice in a row. There are some aspects that differentiate both localizers, such as block quantities, stimuli, and duration time. However, it remains unclear if the localizers will be used in future scanning sessions. For this reason, we will not describe the relevant differences here. 

$\hspace{1cm}$ The stimuli will be presented with a custom-made script using Psychtoolbox-3 [@Brainard1997psychophysics] in MATLAB. Moreover, subjects will be able to view the stimuli due to a mirror that projected the image into a screen inside of the head coil component.

$\hspace{1cm}$ \textbf{\textit{Imaging parameters.}} The data will be collected using a 3-T Philips scanner with a 32-channel head coil in the Department of Radiology of the University Hospital Leuven. We plan on acquiring the functional images using a two-dimensional (2D) multiband (MB) T2*-weighted echo planar imaging sequence with an MB of 2, time repetition (TR) of 2 s, 30 ms echo time (TE), 90º flip angle, 60 transverse slices and a voxel size of 2x2x2 $mm^3$. A high-resolution T1-weighted structural scan will also be obtained from each subject using an MP-RAGE pulse sequence (1 x 0.89 x 0.89 $mm^3$ voxel size). 

$\hspace{1cm}$ \textbf{\textit{Preprocessing.}} The blood-oxygen-level-dependent (BOLD) signal captured with fMRI is contaminated with noise. Preprocessing is a crucial step in hemodynamic imaging to identify the unwanted artefacts and convert the data into a standardized format [@park2019]. In the current study, fMRIPrep version 20.2.0 [@esteban2019] will be used, with the default settings \footnote{With the following exceptions: slice-timing correction will be skipped, as well as the FreeSurfer reconstruction. The reconstructions for the pilot participant’s T1-weighted anatomical scans were previously achieved with FastSurfer (Henschel et al., 2020).}, for preprocessing the anatomical and functional data. The T1-weighted image will be corrected for intensity non-uniformity, skull-stripped, brain tissue segmentation, and go through nonlinear volume-based registration to ICBM 152 nonlinear Asymmetrical template version 2009c for normalization. Each of the bold runs will be motion-corrected, coregistrered to the individual’s anatomical scan, and normalized into standard space MNI152NLin2009cAsym. In the first pilot session, there was no extensive head motion, but a frame-wise displacement spike threshold of 0.5 (i.e., half voxel size) was set. Future participants’ runs that exceed this threshold will be excluded (for more information on fMRIPrep, see Supplementary Materials).

$\hspace{1cm}$ The other necessary steps will be conducted in MATLAB with the established procedure, as outlined by @poldrack2011, using the Statistical Parametric Mapping (SPM; v7771) software package SPM12 (Wellcome Trust Centre for Neuroimaging in London). The functional volumes will be smoothed with a Gaussian kernel, 4 mm FWHM. This entails that the smoothing function follows a Gaussian distribution; the width and height represent how much we will smooth the voxels around the voxel at hand (Poldrack et al., 2011). 

$\hspace{1cm}$ Once all the necessary preprocessing steps are accomplished, general linear models (GLMs) will be applied to obtain the specific beta values in each voxel for all stimuli presented in the experimental runs and all stimuli in the localizer runs. In the case of one pilot session, for each run, the GLMs produced a beta value (contained as in the beta-image) of each regressor of interest and nuisance regressor. More specifically, all the experimental runs consisted of forty regressors of interest (the boxcar functions were derived from the onset timing and the duration of each regressor of interest, and convolved with the gamma distribution to obtain the hemodynamic response function (HRF) for each stimulus) and seven nuisance regressors (six head motion correction parameters and one global signal). Meanwhile, the localizer runs consisted of four regressors of interest (the boxcar functions for each of these regressors of interest were derived using the block onset timings and durations, and convolved with the gamma distribution to obtain the category-specific HRF) and the same seven nuisance regressors. For the non-expert reader, the boxcar function basically represents the timing of a specific stimulus, whereas the convolution with the gamma distribution is used as a mathematical approximation to the BOLD signal response over time. In this way a model of the brain activation is predicted and used to compare with the real observed fMRI data [@Macey2016].

$\hspace{1cm}$ \textbf{\textit{Regions of interest (ROIs).}} For this study, we will use Marsbar and WFU PickAtlas Toolbox to define ROIs. Marsbar is a toolbox for SPM that provides easy-to-use interface in order to define ROIs and extracting data from those same ROIs. Meanwhile, the WFU PickAtlas allows to customize masks based on available anatomical atlases. For the pilot sessions, multiple masks of each ROI were built to compare the neural data from each of these and to investigate whether the constrains of the ROIs by localizer activations were necessary. The masks were built for each ROI from the average MNI coordinates based on previous literature [@bona2014causal; @bai2009abnormal; @dillen2016aberrant; @kovacs2014all; @schobert2018functional; @wang2019individual], except the DLPFC mask (see Table 3). This latter encompasses a relatively large area which was not delineated through coordinates, but through combining Brodmann’s areas 9 and 46 using WFU Pickatlas. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#df with ROIs information
df <- data.frame(
  ROI = c("FFA", "LOC", "PPA", "TPJ", "PCC"),
  MNI_Coordinates = c("-38, -58, -14 / 40, -55, -12", "-44, -77, -12 / 44, -78, -13", "-30, -50, -10 / 30, -54, -12", "-56, -47, 33 / 56, -47, 33", "2, -30, 34"),
  Source = c("Schobert et al. (2018)", "Bona et al. (2014)", "Wang et al. (2019)", "Kovács et al. (2014)", "Dillen et al. (2016)"),
  Information = c("N = 15", "N = 15", "N = 21", "Peak activations across 26 studies", "Peak coordinates Bai et al. (2009)")
)

#adding the L/R information
colnames(df)[colnames(df) == 'MNI_Coordinates'] <- 'MNI Coordinates (L / R)'

#print
df %>%
  kable("latex", caption = "MNI coordinates for each ROI") %>%
  kable_styling(bootstrap_options = "striped", full_width = F) 
```

\begin{figure}
\centering
\makebox[\textwidth][c]{\includegraphics[width=0.8\textwidth]{roimasks.png}}
\captionsetup{labelformat=empty, font=scriptsize, justification=raggedright}
\caption[\textbf{\textit{Figure 9.}} Masks built for each ROI out of the average MNI coordinates.]{\textbf{\textit{Figure 9.} Masks built for each ROI out of the average MNI coordinates with a 10mm sphere around it.} The ROIs shown here are the PCC, FFA, LOC, PPA, and TPJ, respectively. These visual overlays were done with MRIcroGL, a software program designed to visualize and interpret magnetic resonance imaging (MRI) data. It is a free and open source tool.}
\end{figure}

$\hspace{1cm}$ \textbf{\textit{Multi-voxel pattern analysis.}} The MVPA analysis will be performed using the CoSMoMVPA toolbox, with a Linear Discriminant Analysis (LDA) classifier employed to differentiate between distinct patterns of brain activity [@oosterhof2016]. The LDA clusters different classes based on a linear combination of parameters that maximizes the separability between them [@treder2020mvpalight]. Moreover, this toolbox provides multiple positive features, such as reproducibility and accessibility. To evaluate the performance of this classifier, in the pilot sessions, we used a 6-fold cross-validation method and computed the average decoding performance across all possible pairs of conditions. This allowed us to see how effectively the LDA classifier could distinguish between each possible pair of stimuli across runs.

$\hspace{1cm}$ \textbf{\textit{Mahalanobis distance matrices.}} For each ROI, the mean distance among all possible stimulus pairs will be computed, utilizing a function that calculates cross-validated distances via the linear discriminant, effectively determining the cross-validated Mahalanobis distance (see Walther et al., 2016 for more explanation). Future RSA will integrate these distance or dissimilarity matrices with theoretical RDMs and computational RDMs. Given the preliminary data in the current study, the distance matrices for solely one participant were rendered as heatmaps to visually illustrate potential patterns in the relationships between the stimuli. Specifically, the study examined whether the neural responses to checkmate stimuli exhibited greater similarities among themselves relative to non-checkmate stimuli.

\subsection*{Preliminary results}
\addcontentsline{toc}{subsection}{Preliminary results}

$\hspace{1cm}$ What to expect from the measured pattern information is to some extent unclear. First, this posits a fundamental question in the realm of how well brain-activity patterns - especially, when measured with multiple techniques - reflect neuronal information patterns. One could even argue in favor of a conceptual reform regarding the concept of representation [@favela2023], but this is out of the scope for this thesis. According to Kriegeskorte et al. (2008), RSA characterizes information patterns by assessing the similarities, which offers an approach to address the aforementioned issue. Despite the challenge to anticipate the pattern information shared by fMRI and neuronal activity, at the current stage the results seem promising. To briefly describe the future steps and expectations, it is crucial to understand the basic computation behind one single RDM: starting from the experimental conditions until the dissimilarity matrix. 

$\hspace{1cm}$ After performing the GLM, a set of beta values is obtained. These values highlight the magnitude of the brain's response to a particular condition. In the three-dimensional brain space (i.e., related to the voxels' x, y, and z dimensions), a beta map is constructed for each visual stimuli. Once this is done, we are able to examine a brain region and record the pattern of beta values within that area for each condition. For instance, in a scenario with four visual images shown (each representing one condition) and with one ROI, we would obtain a ROI voxel matrix, where each row of the matrix represents a stimulus, and each column represents a voxel within the ROI. This allows us to then look at the dissimilarity patterns (or similarity) for each pair of the four conditions. The values obtained from this comparison serve to assemble a dissimilarity matrix (Kriegeskorte et al., 2008). This exact procedure is done for each subject. The multiple RDMs, therefore, can be compared between themselves (i.e., correlation distance, euclidean distance, and mahalanobis distance, as mentioned in Kriegeskorte et al., 2008). By doing this, we attempt to establish a second-order isomorphism [@shepard1970] - relationship between the associations among the stimuli and those among their representations (match of the dissimilarity matrices) - instead of a first order-isomorphism, which stands for the direct correspondence of a stimulus and its representation. 

$\hspace{1cm}$ For the assessment of whether our study has enough sensitivity to explore our key hypotheses, we used the cross-validated Mahalanobis distance to figure out the average difference between conditions for each ROI for our first pilot participant. According to previous research by @walther2016reliability, this is a trustworthy method for measuring dissimilarity. As a result, we were able to get a dissimilarity matrix for each ROI. Kriegeskorte et al. (2008) mentioned that model (dis)similarity matrices can also be generated from theoretical frameworks that describe the way that a particular brain region might represent a certain information. These conceptual models are not derived from actual activity patterns, but still are useful to compare hypothetical similarity structures (see Figure 10). Indeed, here the heatmap of the theoretical model specifies the structure we would expect to find, to some extent, in experts. That is, the positions with checkmate are more similar among themselves despite the visual similarity non-checkmate versions (Figure 10D). Moreover, the second conceptual matrix demonstrates the visual similarity between positions (Figure 10C). We plan on further computing different types of matrices, such as the actual brain representations and from computational models. These will serve for future comparisons. For illustration, one might observe some patterns delineation in the heatmap of the neural dissimilarity matrix for the LOC (Figure 10A) and DLPFC (Figure 10B), but this current data is based solely on one participant. Thus, the data and its neural dissimilarity matrices contain a lot of noise. 

\begin{figure}
\centering
\makebox[\textwidth][c]{\includegraphics[width=0.8\textwidth]{neuramatricesandcocneptual.png}}
\captionsetup{labelformat=empty, font=scriptsize, justification=raggedright}
\caption[\textbf{\textit{Figure 10.}} Heatmap of the dissimilarity matrices.]{\textbf{\textit{Figure 10.} Heatmap of the dissimilarity matrices.} This figure illustrates the cross-validated Mahalanobis Distance of the LOC, DLPFC, visual similarity, and checkmate versus non-checkmate pattern, respectively.}
\end{figure}

$\hspace{1cm}$ With these participants- and ROI-specific dissimilarity matrices, we will be able to investigate the distinctiveness of the response pattern in each group. To test if reliable information is present in the response patterns, we will look whether the average Mahalanobis distance is higher than zero. More specifically, this will be achieved by determining the differences between within-condition and between-condition dissimilarities for each matrix. In the end, we will also conduct \textit{t}-tests to compare the group-specific outcomes. Depending on the differences between the group values and if we establish that experts demonstrate a unique neural response pattern in a specific ROI, this could suggest the proficiency-related refinement of the representations in experts. Accordingly, Duyck et al. (2021) demonstrated that enhancement might be seen as distinctiveness and consistency, which are not redundant. Consistency is related to the similarity between participants, whereas distinctiveness is connected to the strength of the neural representations within participants. 

\subsection*{Discussion}
\addcontentsline{toc}{subsection}{Discussion}

$\hspace{1cm}$ Our preliminary results highlight interesting aspects. The pilot sessions point us towards a promising future. The decoding performances across our ROIs seem to suggest that we will be able to investigate group-differences. The observed dissimilarity matrix also provides added value to the current methodology. As Duyck et al. (2021) found evidence for a rich representational similarity structure, our current study points us toward a similar end result. In fact, one may argue that this current study could provide even stronger evidence for this structure. Our experimental task induces expertise-related behavior which was not necessarily the case in Duyck et al. (2021) as participants were free to judge birds' similarities based on any way they judged to be the most fitting. 

\sectionfont{\centering}
\begingroup
\Large
\bfseries
\section*{General discussion}
\addcontentsline{toc}{section}{General discussion}
\endgroup

$\hspace{1cm}$ This study will allows us to significantly extend the knowledge concerning expertise. This is crucial given that we can better comprehend how the neural representations change throughout learning. Moreover, investigating experts, both at the behavioral and neural level, provides us with multiple ways of examining precisely the way in which information processing modulates the human brain from a domain-general system to a tailored domain. 

$\hspace{1cm}$ In order to contribute to the current state-of-art, we created a set of stimuli that differentiated the performance of experts and intermediates. Following the behavioral task, we designed and tested the fMRI experiment with four pilot participants. The results obtained from the behavioural study and the reported data from one of the pilot sessions positively highlights a well-designed study.

\vspace{0.2cm}

\subsection*{Strengths}
\addcontentsline{toc}{subsection}{Strengths}

$\hspace{1cm}$ The design of the behavioral experiment considered some caveats that feature in chess-related research (Van Der Maas & Wagenmakers, 2005), such as the choose-the-best-move approach. Participants were not instructed to find the best move available, but to simply evaluate a position and find the checkmate if available. Related to that, we also tested the level of difficulty posed by the multiple different questions instead of solely assuming this difficulty would exist. 

$\hspace{1cm}$ In terms of the participants, the experts' in the behavioral experiment were representative of our expectations for the future fMRI sessions (i.e., around 2000-2200 Elo). The requirement for a minimum Elo rating to be classified as an expert does not preclude that there is a qualitative (and quantitative) difference between different levels of expertise within experts. For instance, in the recent years, there is quite a significant amount of players achieving the level of "super" grandmasters, but these are probably unlikely to be participants for the fMRI experiment. Moreover, concerning the intermediates, one may argue that our results would be even more reliable when comparing novices to experts. Indeed, it seems reasonable to assume that novices would underperform in comparison to intermediates.

$\hspace{1cm}$ Last, the fMRI study facilitates the extension of the behavioral findings to the neural level, which enhances our understanding of chess expertise. More importantly, this combination of behavioral and neuroimaging techniques allows us to achieve external validity [@gegenfurtner2017]. Thus, we implemented very similar tasks for both experiments, ensuring a more direct comparison between the level of analysis at the behavioral and neural level. This is in line with recent research (Martens et al., 2018; Duyck et al., 2021) that has surpassed past studies and prompts a linear comparison between the representation of objects of expertise. Notably, this is the first study to investigate chess expertise at the subordinate level.

\subsection*{Limitations}
\addcontentsline{toc}{subsection}{Limitations}

$\hspace{1cm}$ The results obtained from the pilot behavioral study posit some limitations. The number of participants was not enough to make more fine-grained analysis. A sample with clearly defined groups (i.e., experts and novices) and a higher number of participants, would provide more robust findings. Hence, the results obtained, especially for the mixed-effects logistic regression, may be inaccurate and underpowered to certain extent, but the choices of tests throughout this thesis were done in order to minimize biases as much as possible. Most importantly, it was solely a pilot study, which is strongly advised for most studies (Lakens, 2022), to ensure the feasibility of the research.

$\hspace{1cm}$ Second, the set of chess positions might not be categorized in the most objective metric as it was a \textit{post-hoc} procedure. Past research has made use of an efficient MOA method in which the participants arrange a set of images based on their visual similarity in a two-dimensional space [@kriegeskorte2012; @Martens2018]. For this study, the similarity between positions was based on the pieces involved in the checkmate sequence and the tactics required, which allowed us to include them in different categories after developing them. This procedure could be done in a different manner, such as using positions that are clearly embedded in a chess-theoretical categorization (i.e., chess openings). This may be more beneficial in order to obtain more well-defined categories, but also has its own limitations.

$\hspace{1cm}$ Last, one intrinsic factor to chess expertise is that top-down factors may underlie on the neural responses to the objects of expertise. Bartlett et al. (2013) argued that the prefrontal-parietal network was involved with a more strategy-based in contrast to a more automatic pattern recognition that happens in chess experts when analyzing normal chess positions. The current study involves stimuli that might require some decision-making instead of only pattern recognition. Therefore, there is no manipulation to clearly disentangle possible top-down mechanisms, even if we included the associated regions, such as the DPLFC. 

\vspace{8 cm}

[@henschel2020fastsurfer];[@Karch2021]; [@Stoffel2021]

\newpage

# References

<div id="refs"></div>

\bibliography{refs.bib}

\newpage 

# Supplementary materials

\begin{figure}
\centering
\makebox[\textwidth][c]{\includegraphics[width=0.8\textwidth]{bayeswilcox.score.png}}
\captionsetup{labelformat=empty, font=scriptsize, justification=raggedright}
\caption[\textbf{\textit{Figure 11.}} Bayesian Mann Whitney U Test for correct answers and response times.]{\textbf{\textit{Figure 11.} Bayesian Mann Whitney U Test for correct answers and response times.} This figure illustrates the inferential plots obtained in JASP for the prior and posterior distributions. Plot A shows the distributions for the correct answers, whereas plot B shows the distributions for response times.}
\end{figure}

\begin{figure}
\centering
\makebox[\textwidth][c]{\includegraphics[width=0.8\textwidth]{pairwisedistances.png}}
\captionsetup{labelformat=empty, font=scriptsize, justification=raggedright}
\caption[\textbf{\textit{Figure 12.}} Pairwise distances.]{\textbf{\textit{Figure 12.} Pairwise distances.} This figure illustrates the pairwise differences within each group.}
\end{figure}

\newpage

```{r, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
library(brms)
#dependency bayes
model_bayesdependency <- brm(Answer ~ Question_Difficulty + rankings + (1|participant),
                   family = bernoulli(link = "logit"), data = data, 
                   prior = c(prior(normal(0, 10), class = "b", coef = "Question_DifficultyEasy"),
                             prior(normal(0, 10), class = "b", coef = "rankings1"),
                             prior(cauchy(0, 2), class = "sd", group = "participant")),
                   warmup = 1000, iter = 20000, chains = 4, cores = 4, seed = 12345,
                   sample_prior = "yes", save_pars = save_pars(all = TRUE))

#model_bayesdependency
plot(model_bayesdependency)

library(ggplot2)

# Create the plot as a ggplot object
p <- plot(model_bayesdependency)

# Alter the theme of the plot to remove the grid lines
p + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())

#posterior predictive check
#plot <- pp_check(model_bayesdependency) + theme_minimal() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Assuming 'fit' is your fitted model
fit <- model_bayesdependency
# Extract fixed effects summary
fixed_effects_summary <- summary(fit)$fixed
# Add term names
fixed_effects_summary$Term <- rownames(fixed_effects_summary)

# Make a data frame
fixed_effects_df <- data.frame(
  Term = fixed_effects_summary$Term,
  Estimate = round(fixed_effects_summary$Estimate, 3),
  SE = round(fixed_effects_summary$Est.Error, 3),
  l_95_CI = round(fixed_effects_summary$`l-95% CI`, 3),
  u_95_CI = round(fixed_effects_summary$`u-95% CI`, 3),
  Rhat = round(fixed_effects_summary$Rhat, 3),
  Bulk_ESS = fixed_effects_summary$Bulk_ESS,
  Tail_ESS = fixed_effects_summary$Tail_ESS,
  stringsAsFactors = FALSE
)

# Change the fixed effect names
names <- c("Intercept", "Easy Questions", "Experts")
fixed_effects_df$Term <- names

# Generate fixed effects table
library(knitr)
library(kableExtra)
fixed_effects_table <- kable(fixed_effects_df, "latex", align = "lcccc", 
      row.names = FALSE, booktabs = TRUE, 
      col.names = c("Term", "Estimate", "SE", "Lower 95% CI", "Upper 95% CI", "Rhat", "Bulk ESS", "Tail ESS"),
      caption = "Fixed effects for bayesian analysis") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

fixed_effects_table
```

\textbf{Bayesian mixed-effects logistic regression}

$\hspace{1cm}$ This analysis was not intended to develop a Bayesian cognitive model tailored for chess performance. Despite previous research showing the detrimental influence that vague priors may have [@lee2018bayesian], we solely conducted this analysis to provide more comprehensive results for the main analysis reported in the thesis. We assigned a normal prior distribution with a mean of 0 and standard deviation of 10 for the difficulty level estimate (i.e., easy compared to hard questions), a normal prior distribution with a mean of 0 and standard deviation of 10 for the group level estimate (i.e., experts compared to intermediates), and a half-Cauchy distribution with a location of 0 and scale parameter of 2 for the standard deviation of the random effects. The results supported our findings (see Table 4).

\vspace{2 cm}

\begin{figure}
\centering
\makebox[\textwidth][c]{\includegraphics[width=0.8\textwidth]{posteriordistributions.png}}
\captionsetup{labelformat=empty, font=scriptsize, justification=raggedright}
\caption[\textbf{\textit{Figure 13.}} Posterior distributions.]{\textbf{\textit{Figure 13.} Posterior distributions.} The plot shows the posterior distributions for the estimates (on the logit scale).}
\end{figure}

```{r descriptive scores, echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)

#columns excluding 'minimum', 'maximum', 'median', 'total_averagert'
nonrankedplayers_selected <- nonrankedplayers %>%
  select(-minimum, -maximum, -median, -total_averagert)

#the same for the ranked players data frame
rankedplayers_selected <- rankedplayers %>%
  select(-minimum, -maximum, -median)

#round all numeric columns to 2 decimal places 
nonrankedplayers_selected[] <- lapply(nonrankedplayers_selected, function(x) if(is.numeric(x)) round(x, 2) else x)

#table
nonrankedplayers_table <- knitr::kable(nonrankedplayers_selected, "latex", align = "lccccc", 
                                       row.names = FALSE, booktabs = TRUE, 
                                       col.names = c("Questions", "Amount of correct answers", "Mean", 
                                                     "SD", "Lower Bound", "Upper Bound"),
                                       caption = "Intermediates’ scores for the behavioral study") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  add_footnote("The lower bound values stand for the mean minus one standard error, while the upper bound values are the mean plus one standard error.")



#print
nonrankedplayers_table

#round all numeric columns to 2 decimal places 
rankedplayers_selected[] <- lapply(rankedplayers_selected, function(x) if(is.numeric(x)) round(x, 2) else x)

#table
rankedplayers_table <- knitr::kable(rankedplayers_selected, "latex", align = "lccccc", 
                                     row.names = FALSE, booktabs = TRUE, 
                                     col.names = c("Questions", "Amount of correct answers", "Mean", 
                                                   "SD", "Lower Bound", "Upper Bound"),
                                     caption = "Experts’ scores for the behavioral study") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  add_footnote("The lower bound values stand for the mean minus one standard error, while the upper bound values are the mean plus one standard error.")


#print
rankedplayers_table
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Load required packages
library(knitr)
library(kableExtra)

# Create a data frame with the given information
dataratings <- data.frame(
  Participant = c("FS03", "FS06", "FS07", "FS08", "FS09", "FS10", "FS11", "FS13", "FS14", "FS15", "FS17", "FS19", "FS20", "FS22", "FS24", "FS25", "FS28", "FS30", "FS33"),
  Rating = c("-", 2442, 2221, "-", 2272, 2212, "-", "-", 2259, 2160, 1800, 1475, "-", 1089, 2200, "-", 2140, "-", 2145),
  stringsAsFactors = FALSE
)

# Create an APA-style table
kable(dataratings, "latex", align = "lc", caption = "Participants' rankings", row.names = FALSE, booktabs = TRUE) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

\newpage

\textbf{Information about the anatomical and functional data preprocessing.}

$\hspace{1cm}$ Results included in this manuscript come from preprocessing performed using fMRIPrep 22.1.1 (Esteban, Markiewicz, et al. (2018); Esteban, Blair, et al. (2018); RRID:SCR_016216), which is based on Nipype 1.8.5 (K. Gorgolewski et al. (2011); K. J. Gorgolewski et al. (2018); RRID:SCR_002502).

$\hspace{1cm}$ \textbf{\textit{Anatomical data preprocessing.}} A total of 1 T1-weighted (T1w) images were found within the input BIDS dataset.The T1-weighted (T1w) image was corrected for intensity non-uniformity (INU) with N4BiasFieldCorrection (Tustison et al. 2010), distributed with ANTs 2.3.3 (Avants et al. 2008, RRID:SCR_004757), and used as T1w-reference throughout the workflow. The T1w-reference was then skull-stripped with a Nipype implementation of the antsBrainExtraction.sh workflow (from ANTs), using OASIS30ANTs as target template. Brain tissue segmentation of cerebrospinal fluid (CSF), white-matter (WM) and gray-matter (GM) was performed on the brain-extracted T1w using fast (FSL 6.0.5.1:57b01774, RRID:SCR_002823, Zhang, Brady, and Smith 2001). Volume-based spatial normalization to one standard space (MNI152NLin2009cAsym) was performed through nonlinear registration with antsRegistration (ANTs 2.3.3), using brain-extracted versions of both T1w reference and the T1w template. The following template was selected for spatial normalization: ICBM 152 Nonlinear Asymmetrical template version 2009c [Fonov et al. (2009), RRID:SCR_008796; TemplateFlow ID: MNI152NLin2009cAsym].

$\hspace{1cm}$ \textbf{\textit{Functional data preprocessing.}} For each of the 9 BOLD runs found per subject (across all tasks and sessions), the following preprocessing was performed. First, a reference volume and its skull-stripped version were generated using a custom methodology of fMRIPrep. Head-motion parameters with respect to the BOLD reference (transformation matrices, and six corresponding rotation and translation parameters) are estimated before any spatiotemporal filtering using mcflirt (FSL 6.0.5.1:57b01774, Jenkinson et al. 2002). The BOLD time-series (including slice-timing correction when applied) were resampled onto their original, native space by applying the transforms to correct for head-motion. These resampled BOLD time-series will be referred to as preprocessed BOLD in original space, or just preprocessed BOLD. The BOLD reference was then co-registered to the T1w reference using mri_coreg (FreeSurfer) followed by flirt (FSL 6.0.5.1:57b01774, Jenkinson and Smith 2001) with the boundary-based registration (Greve and Fischl 2009) cost-function. Co-registration was configured with six degrees of freedom. Several confounding time-series were calculated based on the preprocessed BOLD: framewise displacement (FD), DVARS and three region-wise global signals. FD was computed using two formulations following Power (absolute sum of relative motions, Power et al. (2014)) and Jenkinson (relative root mean square displacement between affines, Jenkinson et al. (2002)). FD and DVARS are calculated for each functional run, both using their implementations in Nipype (following the definitions by Power et al. 2014). The three global signals are extracted within the CSF, the WM, and the whole-brain masks. Additionally, a set of physiological regressors were extracted to allow for component-based noise correction (CompCor, Behzadi et al. 2007). Principal components are estimated after high-pass filtering the preprocessed BOLD time-series (using a discrete cosine filter with 128s cut-off) for the two CompCor variants: temporal (tCompCor) and anatomical (aCompCor). tCompCor components are then calculated from the top 2% variable voxels within the brain mask. For aCompCor, three probabilistic masks (CSF, WM and combined CSF+WM) are generated in anatomical space. The implementation differs from that of Behzadi et al. in that instead of eroding the masks by 2 pixels on BOLD space, a mask of pixels that likely contain a volume fraction of GM is subtracted from the aCompCor masks. This mask is obtained by thresholding the corresponding partial volume map at 0.05, and it ensures components are not extracted from voxels containing a minimal fraction of GM. Finally, these masks are resampled into BOLD space and binarized by thresholding at 0.99 (as in the original implementation). Components are also calculated separately within the WM and CSF masks. For each CompCor decomposition, the k components with the largest singular values are retained, such that the retained components’ time series are sufficient to explain 50 percent of variance across the nuisance mask (CSF, WM, combined, or temporal). The remaining components are dropped from consideration. The head-motion estimates calculated in the correction step were also placed within the corresponding confounds file. The confound time series derived from head motion estimates and global signals were expanded with the inclusion of temporal derivatives and quadratic terms for each (Satterthwaite et al. 2013). Frames that exceeded a threshold of 0.5 mm FD or 1.5 standardized DVARS were annotated as motion outliers. Additional nuisance timeseries are calculated by means of principal components analysis of the signal found within a thin band (crown) of voxels around the edge of the brain, as proposed by (Patriat, Reynolds, and Birn 2017). The BOLD time-series were resampled into standard space, generating a preprocessed BOLD run in MNI152NLin2009cAsym space. First, a reference volume and its skull-stripped version were generated using a custom methodology of fMRIPrep. All resamplings can be performed with a single interpolation step by composing all the pertinent transformations (i.e. head-motion transform matrices, susceptibility distortion correction when available, and co-registrations to anatomical and output spaces). Gridded (volumetric) resamplings were performed using antsApplyTransforms (ANTs), configured with Lanczos interpolation to minimize the smoothing effects of other kernels (Lanczos 1964). Non-gridded (surface) resamplings were performed using mri_vol2surf (FreeSurfer).

$\hspace{1cm}$ Many internal operations of fMRIPrep use Nilearn 0.9.1 (Abraham et al. 2014, RRID:SCR_001362), mostly within the functional processing workflow. For more details of the pipeline, see the section corresponding to workflows in fMRIPrep’s documentation.

$\hspace{1cm}$ \textbf{Copyright Waiver}

$\hspace{1cm}$ The above boilerplate text was automatically generated by fMRIPrep with the express intention that users should copy and paste this text into their manuscripts unchanged. It is released under the CC0 license.

\newpage

# Appendix 

$\hspace{1cm}$ This document was created to show visually the positions used for the behavioral pilot study described in section I. Here we also present the correct moves' sequences that lead to a checkmate for each position. The disposition of the images is according to the randomized order shown in the online experiment. Moreover, the categorization developed (i.e., pieces involved in the checkmate sequence and the tactics in-play) is highlighted. As discussed in the manuscript, this is by no means a definitive categorization procedure. For instance, one could do it based solely on the tactics, the number of pieces involved, and others.

$\hspace{1cm}$ Every position entails a checkmate in three or four moves for white. Some positions, however, can also achieve a checkmate for white with more moves — those are mentioned in this document. The evaluation bar next to the positions without checkmate are based on the evaluation from Stockfish 11 (search depth = 20). Stockfish is a chess engine. The version 11 was used to evaluate the positions and the search-depth parameter refers to the amount of moves analyzed by Stockfish. 

\vspace{0.5cm}

\textbf{First category:} \textit{Queen/rook checkmate patterns involving similar tactics.} Positions 1, 25, and 11

\textbf{Second category:} \textit{Queen/rook checkmate patterns while receiving support from the bishop/knight.} Positions 2, 9, 21, and 17

\textbf{Third category:} \textit{Knight/bishop restraining the king.} Positions 5, 8, 14, 23, 19, 13, 4, and 7

\textbf{Fourth category:} \textit{Bishop forcing moves.} Positions 10, 16, 20, 26, and 18

\textbf{Fifth category:} \textit{One move checkmate.} Positions 3, 6, 12, 15, 22, and 24
